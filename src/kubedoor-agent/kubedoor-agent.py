import asyncio
import base64
import json
import sys
from urllib.parse import urlencode
from aiohttp import ClientSession, ClientWebSocketResponse, WSMsgType, web
from kubernetes_asyncio import client, config
from kubernetes_asyncio.client.rest import ApiException
from datetime import datetime, timedelta, timezone
from loguru import logger
import utils
import configmap_manager
import service_manager
import ingress_manager
import pod_manager
import istio_manager
import re
from deployment_monitor import DeploymentMonitor
from k8s_event_monitor import K8sEventMonitor
from event_monitor_config import *
from k8s_node_scheduler import K8sNodeScheduler
from k8s_client_manager import K8sClientManager
from node_manager import get_nodes_list, cordon_nodes, uncordon_nodes
import k8s_resource_handler
import stateful_daemon_manager

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stderr,
    format='<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> [<level>{level}</level>] <level>{message}</level>',
    level='INFO',
)

VERSION = utils.get_version()
# å…¨å±€å˜é‡
ws_conn = None
v1 = None  # AppsV1Api
batch_v1 = None  # BatchV1Api
core_v1 = None  # CoreV1Api
networking_v1 = None  # NetworkingV1Api
admission_api = None  # AdmissionregistrationV1Api
custom_api = None  # CustomObjectsApiï¼ˆç”¨äºè®¿é—®Metrics APIï¼‰
deployment_monitor = None  # DeploymentMonitorå®ä¾‹
event_monitor = None  # K8sEventMonitorå®ä¾‹
# ç”¨äºå­˜å‚¨ WebSocket è¯·æ±‚çš„ Future
request_futures = {}
# å­˜å‚¨Podæ—¥å¿—æµä»»åŠ¡
pod_logs_tasks = {}


def init_kubernetes():
    """åœ¨ç¨‹åºå¯åŠ¨æ—¶åŠ è½½ Kubernetes é…ç½®å¹¶åˆå§‹åŒ–å®¢æˆ·ç«¯"""
    global v1, batch_v1, core_v1, networking_v1, admission_api, custom_api, deployment_monitor, event_monitor
    try:
        config.load_incluster_config()
        v1 = client.AppsV1Api()
        batch_v1 = client.BatchV1Api()
        core_v1 = client.CoreV1Api()
        networking_v1 = client.NetworkingV1Api()
        admission_api = client.AdmissionregistrationV1Api()
        custom_api = client.CustomObjectsApi()
        deployment_monitor = DeploymentMonitor(v1, core_v1)
        event_monitor = K8sEventMonitor(core_v1)
        logger.info("Kubernetes é…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"åŠ è½½ Kubernetes é…ç½®å¤±è´¥: {e}")
        raise


async def handle_http_request(
    ws: ClientWebSocketResponse, request_id: str, method: str, query: dict, body: dict, path: str
):
    """å¼‚æ­¥å¤„ç† HTTP è¯·æ±‚å¹¶å‘é€å“åº”"""
    try:
        async with ClientSession() as session:
            logger.info(f"è½¬å‘è¯·æ±‚: {method} {path}?{urlencode(query)}ã€{json.dumps(body)}ã€‘")
            if method == "GET":
                async with session.get(path, params=query, ssl=False) as resp:
                    response_data = await resp.json()
            elif method == "POST":
                async with session.post(path, params=query, json=body, ssl=False) as resp:
                    response_data = await resp.json()
            elif method == "DELETE":
                async with session.delete(path, params=query, json=body, ssl=False) as resp:
                    response_data = await resp.json()
            else:
                response_data = {"success": False, "error": f"agentæ”¶åˆ°masterå‘æ¥çš„ä¸æ”¯æŒçš„è¯·æ±‚æ–¹æ³•: {method}"}
                logger.error(response_data["error"])
    except Exception as e:
        response_data = {"success": False, "error": str(e)}
        logger.error(response_data["error"])

    await ws.send_json({"type": "response", "request_id": request_id, "response": response_data})


async def stream_pod_logs(
    ws: ClientWebSocketResponse, connection_id: str, namespace: str, pod_name: str, container: str = ""
):
    """æµå¼è·å–Podæ—¥å¿—å¹¶å‘é€ç»™master"""
    try:
        logger.info(f"å¼€å§‹è·å–Podæ—¥å¿—: {namespace}/{pod_name}")

        # å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯
        await ws.send_json({"type": "pod_logs", "connection_id": connection_id, "status": "connected"})

        # ä½¿ç”¨kubernetes_asyncioçš„æ—¥å¿—æµAPI
        log_stream = await core_v1.read_namespaced_pod_log(
            name=pod_name,
            namespace=namespace,
            container=container if container else None,
            follow=True,
            tail_lines=100,
            timestamps=False,
            _preload_content=False,
        )

        # æµå¼è¯»å–æ—¥å¿—
        buffer = ""
        async for chunk in log_stream.content:
            if chunk:
                try:
                    buffer += chunk.decode('utf-8', errors='ignore')
                    lines = buffer.split('\n')
                    buffer = lines[-1]  # ä¿ç•™æœ€åä¸€è¡Œï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰

                    for line in lines[:-1]:
                        if line.strip():
                            # ç›´æ¥å‘é€çº¯å‡€çš„æ—¥å¿—å†…å®¹ï¼Œä¸åŒ…è£…æˆJSON
                            await ws.send_str(line)
                except Exception as decode_error:
                    logger.warning(f"è§£ç æ—¥å¿—è¡Œå¤±è´¥: {decode_error}")
                    continue

    except asyncio.CancelledError:
        logger.info(f"Podæ—¥å¿—æµè¢«å–æ¶ˆ: {connection_id}")
        await ws.send_json({"type": "pod_logs", "connection_id": connection_id, "status": "disconnected"})
    except ApiException as e:
        error_msg = f"Kubernetes APIé”™è¯¯: {e.status} - {e.reason}"
        logger.error(f"Podæ—¥å¿—æµAPIå¼‚å¸¸: {connection_id}, é”™è¯¯: {error_msg}")
        await ws.send_json({"type": "pod_logs", "connection_id": connection_id, "error": error_msg})
    except Exception as e:
        logger.error(f"Podæ—¥å¿—æµå¼‚å¸¸: {connection_id}, é”™è¯¯: {e}")
        await ws.send_json({"type": "pod_logs", "connection_id": connection_id, "error": str(e)})
    finally:
        # æ¸…ç†ä»»åŠ¡
        if connection_id in pod_logs_tasks:
            del pod_logs_tasks[connection_id]


async def process_request(ws: ClientWebSocketResponse):
    """å¤„ç†æœåŠ¡ç«¯å‘é€çš„è¯·æ±‚"""
    async for msg in ws:
        if msg.type == WSMsgType.TEXT:
            try:
                data = json.loads(msg.data)
            except json.JSONDecodeError:
                logger.error(f"æ”¶åˆ°æ— æ³•è§£æçš„æ¶ˆæ¯ï¼š{msg.data}")
                continue
            if data.get("type") == "admis":
                request_id = data.get("request_id")
                deploy_res = data.get("deploy_res")
                logger.info(f"æ”¶åˆ° admis æ¶ˆæ¯ï¼š{request_id} {deploy_res}")
                if request_id in request_futures:
                    request_futures[request_id].set_result(deploy_res)
                    del request_futures[request_id]
            elif data.get("type") == "request":
                request_id = data["request_id"]
                method = data["method"]
                query = data["query"]
                body = data["body"]
                path = (
                    'http://127.0.0.1:81' + data["path"]
                    if data["path"].startswith('/api/pod/')
                    else 'https://127.0.0.1' + data["path"]
                )
                asyncio.create_task(handle_http_request(ws, request_id, method, query, body, path))
            elif data.get("type") == "start_pod_logs":
                # å¼€å§‹Podæ—¥å¿—æµ
                connection_id = data.get("connection_id")
                namespace = data.get("namespace")
                pod_name = data.get("pod_name")
                container = data.get("container", "")
                logger.info(f"å¼€å§‹Podæ—¥å¿—æµ: {connection_id}")
                task = asyncio.create_task(stream_pod_logs(ws, connection_id, namespace, pod_name, container))
                pod_logs_tasks[connection_id] = task
            elif data.get("type") == "stop_pod_logs":
                # åœæ­¢Podæ—¥å¿—æµ
                connection_id = data.get("connection_id")
                logger.info(f"åœæ­¢Podæ—¥å¿—æµ: {connection_id}")
                if connection_id in pod_logs_tasks:
                    pod_logs_tasks[connection_id].cancel()
                    del pod_logs_tasks[connection_id]
        elif msg.type == WSMsgType.ERROR:
            logger.error(f"WebSocket é”™è¯¯ï¼š{msg.data}")


async def heartbeat(ws: ClientWebSocketResponse):
    """å®šæœŸå‘é€å¿ƒè·³"""
    while True:
        try:
            await ws.send_json({"type": "heartbeat"})
            logger.debug("æˆåŠŸå‘é€å¿ƒè·³")
            await asyncio.sleep(HEARTBEAT_INTERVAL)
        except Exception as e:
            logger.error(f"å¿ƒè·³å‘é€å¤±è´¥ï¼š{e}")
            break


async def monitor_health_check():
    """å®šæœŸå¥åº·æ£€æŸ¥ï¼Œç›‘æ§äº‹ä»¶ä¼ è¾“çŠ¶æ€"""
    last_check_time = datetime.now()
    while True:
        try:
            await asyncio.sleep(HEALTH_CHECK_INTERVAL)  # å¥åº·æ£€æŸ¥é—´éš”

            current_time = datetime.now()

            # æ£€æŸ¥WebSocketè¿æ¥å¥åº·çŠ¶æ€
            if not event_monitor.is_websocket_healthy():
                logger.warning("âš ï¸ å¥åº·æ£€æŸ¥: WebSocketè¿æ¥ä¸å¥åº·")
                raise Exception("WebSocketè¿æ¥ä¸å¥åº·")

            # æ£€æŸ¥äº‹ä»¶ç›‘æ§çŠ¶æ€
            if not event_monitor.is_running:
                logger.warning("âš ï¸ å¥åº·æ£€æŸ¥: äº‹ä»¶ç›‘æ§æœªè¿è¡Œ")
                raise Exception("äº‹ä»¶ç›‘æ§æœªè¿è¡Œ")

            # æ£€æŸ¥æ˜¯å¦é•¿æ—¶é—´æ²¡æœ‰äº‹ä»¶ï¼ˆå¯èƒ½è¡¨ç¤ºK8säº‹ä»¶æµæ–­å¼€ï¼‰
            if event_monitor.last_event_time:
                time_since_last_event = current_time - event_monitor.last_event_time
                if time_since_last_event.total_seconds() > EVENT_TIMEOUT_THRESHOLD:
                    logger.warning(f"âš ï¸ å¥åº·æ£€æŸ¥: å·²æœ‰ {time_since_last_event.total_seconds():.0f} ç§’æ²¡æœ‰æ”¶åˆ°K8säº‹ä»¶")

            # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            time_since_last_check = current_time - last_check_time
            if time_since_last_check.total_seconds() > STATS_REPORT_INTERVAL:
                logger.debug(
                    f"ğŸ“Š äº‹ä»¶ç›‘æ§çŠ¶æ€: å·²å¤„ç† {event_monitor.event_count} ä¸ªäº‹ä»¶, WebSocketå¥åº·: {event_monitor.is_websocket_healthy()}"
                )
                last_check_time = current_time

        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥ï¼š{e}")
            break


async def connect_to_server():
    """è¿æ¥åˆ° WebSocket æœåŠ¡ç«¯ï¼Œå¹¶å¤„ç†è¿æ¥æ–­å¼€çš„æƒ…å†µ"""
    uri = f"{utils.KUBEDOOR_MASTER}/ws?env={utils.PROM_K8S_TAG_VALUE}&ver={VERSION}"
    while True:
        try:
            async with ClientSession() as session:
                async with session.ws_connect(uri) as ws:
                    logger.info("æˆåŠŸè¿æ¥åˆ°æœåŠ¡ç«¯")
                    global ws_conn
                    ws_conn = ws

                    # è®¾ç½®äº‹ä»¶ç›‘å¬å™¨çš„WebSocketè¿æ¥
                    event_monitor.set_websocket_connection(ws)

                    # å¹¶å‘è¿è¡Œè¯·æ±‚å¤„ç†ã€å¿ƒè·³å‘é€ã€äº‹ä»¶ç›‘å¬å’Œå¥åº·æ£€æŸ¥ï¼Œä½¿ç”¨return_when=FIRST_EXCEPTION
                    # è¿™æ ·ä»»ä½•ä¸€ä¸ªä»»åŠ¡å¼‚å¸¸éƒ½ä¼šå¯¼è‡´é‡æ–°è¿æ¥ï¼Œè€Œä¸æ˜¯æ•´ä¸ªç¨‹åºå´©æºƒ
                    try:
                        done, pending = await asyncio.wait(
                            [
                                asyncio.create_task(process_request(ws)),
                                asyncio.create_task(heartbeat(ws)),
                                asyncio.create_task(event_monitor.start_monitoring()),
                                asyncio.create_task(monitor_health_check()),
                            ],
                            return_when=asyncio.FIRST_EXCEPTION,
                        )

                        # å–æ¶ˆæ‰€æœ‰å¾…å¤„ç†çš„ä»»åŠ¡
                        for task in pending:
                            task.cancel()
                            try:
                                await task
                            except asyncio.CancelledError:
                                pass

                        # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡æ˜¯å¦æœ‰å¼‚å¸¸
                        for task in done:
                            if task.exception():
                                logger.error(f"ä»»åŠ¡å¼‚å¸¸: {task.exception()}")
                                raise task.exception()

                    except Exception as task_e:
                        logger.error(f"WebSocketä»»åŠ¡å¼‚å¸¸: {task_e}")
                        # åœæ­¢äº‹ä»¶ç›‘æ§
                        await event_monitor.stop_monitoring()
                        # æ¸…ç©ºWebSocketè¿æ¥å¼•ç”¨
                        event_monitor.set_websocket_connection(None)
                        ws_conn = None
                        raise task_e

        except Exception as e:
            logger.error(f"è¿æ¥åˆ°æœåŠ¡ç«¯å¤±è´¥ï¼š{e}")
            # ç¡®ä¿æ¸…ç†èµ„æº
            if event_monitor:
                await event_monitor.stop_monitoring()
                event_monitor.set_websocket_connection(None)
            ws_conn = None
            logger.info(f"ç­‰å¾… {WEBSOCKET_RECONNECT_DELAY} ç§’åé‡æ–°è¿æ¥...")
            await asyncio.sleep(WEBSOCKET_RECONNECT_DELAY)


async def delete_cronjob_or_not(cronjob_name, job_type):
    """åˆ¤æ–­æ˜¯å¦æ˜¯ä¸€æ¬¡æ€§ jobï¼Œæ˜¯çš„è¯åˆ é™¤"""
    if job_type == "once":
        try:
            await batch_v1.delete_namespaced_cron_job(
                name=cronjob_name, namespace="kubedoor", body=client.V1DeleteOptions()
            )
            logger.info(f"CronJob '{cronjob_name}' deleted successfully.")
        except ApiException as e:
            logger.exception(f"åˆ é™¤ CronJob '{cronjob_name}' æ—¶å‡ºé”™: {e}")
            utils.send_msg(f"Error when deleting CronJob 'ã€{utils.PROM_K8S_TAG_VALUE}ã€‘{cronjob_name}'!")


async def health_check(request):
    return web.json_response({"ver": VERSION, "status": "healthy"})


async def update_image(request):
    try:
        data = await request.json()
        new_image_tag = data.get('image_tag')
        deployment_name = data.get('deployment')
        namespace = data.get('namespace')

        deployment = await v1.read_namespaced_deployment(deployment_name, namespace)
        current_image = deployment.spec.template.spec.containers[0].image
        image_name = current_image.split(':')[0]
        new_image = f"{image_name}:{new_image_tag}"
        deployment.spec.template.spec.containers[0].image = new_image

        await v1.patch_namespaced_deployment(name=deployment_name, namespace=namespace, body=deployment)

        # å¯åŠ¨åå°ç›‘æ§ä»»åŠ¡
        if deployment_monitor:
            asyncio.create_task(deployment_monitor.monitor_deployment_update(namespace, deployment_name, new_image))

        return web.json_response(
            {
                "success": True,
                "message": f"{namespace} {deployment_name} updated with image {new_image}",
            }
        )
    except Exception as e:
        return web.json_response({"message": str(e)}, status=500)


async def scale(request):
    """æ‰¹é‡æ‰©ç¼©å®¹"""
    request_info = await request.json()
    interval = request.query.get("interval")
    add_label = request.query.get("add_label")
    res_type = request.query.get("type")
    temp = request.query.get("temp")
    isolate = request.query.get("isolate")
    scheduler = request.query.get("scheduler", "false")
    error_list = []

    for index, deployment in enumerate(request_info.get('deployment_list', [])):
        namespace = deployment.get("namespace")
        deployment_name = deployment.get("deployment_name")
        num = deployment.get("num")
        job_name = deployment.get("job_name")
        job_type = deployment.get("job_type")
        logger.info(f"ã€{namespace}ã€‘ã€{deployment_name}ã€‘: {num}")
        nodes = await core_v1.list_node()

        try:
            deployment_obj = await v1.read_namespaced_deployment(deployment_name, namespace)
            if not deployment_obj:
                reason = f"æœªæ‰¾åˆ°ã€{namespace}ã€‘ã€{deployment_name}ã€‘"
                logger.error(reason)
                error_list.append({'namespace': namespace, 'deployment_name': deployment_name, 'reason': reason})
                continue

            # åˆ¤æ–­æ‰©å®¹è¿˜æ˜¯ç¼©å®¹
            current_replicas = deployment_obj.spec.replicas
            logger.info(f"å½“å‰å‰¯æœ¬æ•°: {current_replicas}")

            # å¦‚æœæ˜¯ä¸´æ—¶æ‰©ç¼©å®¹ï¼Œæ·»åŠ annotations
            if temp == 'true':
                if deployment_obj.metadata.annotations is None:
                    deployment_obj.metadata.annotations = {}
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                deployment_obj.metadata.annotations['scale.temp'] = f"{current_time}@{current_replicas}-->{num}"
                logger.info(f"æ·»åŠ ä¸´æ—¶æ‰©ç¼©å®¹æ ‡è®°: {current_time}@{current_replicas}-->{num}")
            else:
                # éä¸´æ—¶æ‰©ç¼©å®¹ï¼Œåˆ é™¤ä¸´æ—¶æ ‡è®°
                del_scale_temp = 0
                if deployment_obj.metadata.annotations and 'scale.temp' in deployment_obj.metadata.annotations:
                    # ä½¿ç”¨patch bodyæ–¹å¼åˆ é™¤æ³¨è§£ï¼Œè®¾ç½®ä¸ºNoneå‘Šè¯‰Kubernetes APIåˆ é™¤è¯¥key
                    deployment_obj.metadata.annotations['scale.temp'] = None
                    del_scale_temp = 1
                    logger.info("åˆ é™¤ä¸´æ—¶æ‰©ç¼©å®¹æ ‡è®°")

            if num > current_replicas and add_label == 'true':
                # å‰¯æœ¬æ•°ä¸èƒ½è¶…è¿‡èŠ‚ç‚¹æ€»æ•°
                if len(nodes.items) < num:
                    return web.json_response(
                        {"message": f"ã€{namespace}ã€‘ã€{deployment_name}ã€‘å‰¯æœ¬æ•°ä¸èƒ½è¶…è¿‡èŠ‚ç‚¹æ€»æ•°"},
                        status=500,
                    )
                node_cpu_list = request_info[0].get("node_cpu_list")  # kubedoor-master scaleæ¥å£è¿½åŠ çš„å˜é‡
                logger.info(f"èŠ‚ç‚¹{res_type}æƒ…å†µ: {node_cpu_list}")
                logger.info(f"æ‰©ç¼©å®¹ç­–ç•¥ï¼šæ ¹æ®ã€èŠ‚ç‚¹{res_type}ã€‘æƒ…å†µï¼Œæ‰§è¡Œæ‰©å®¹ï¼Œç›®æ ‡å‰¯æœ¬æ•°: {num}")
                # åˆ¤æ–­å·²æœ‰æ ‡ç­¾æ•°
                labeled_nodes_count = await get_labeled_nodes_count(namespace, deployment_name, nodes)
                if isolate == 'true':
                    add_isolate = 1
                else:
                    add_isolate = 0
                if labeled_nodes_count < num + add_isolate:
                    nodes_to_label_count = num + add_isolate - labeled_nodes_count
                    # é€‰æ‹©å½“å‰ CPU ä½¿ç”¨ç‡æœ€ä½çš„èŠ‚ç‚¹ï¼Œç›´åˆ°æ»¡è¶³æ‰©å®¹åçš„å‰¯æœ¬æ•°
                    available_nodes = await select_least_loaded_nodes(
                        namespace, nodes_to_label_count, deployment_name, node_cpu_list
                    )

                    if available_nodes:
                        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ æ ‡ç­¾
                        for node in available_nodes:
                            await update_node_with_label(namespace, node, deployment_name)
                    else:
                        reason = "å‰©ä½™å¯è°ƒåº¦èŠ‚ç‚¹ä¸è¶³"
                        logger.error(reason)
                        error_list.append(
                            {
                                'namespace': namespace,
                                'deployment_name': deployment_name,
                                'reason': reason,
                            }
                        )
                        return web.json_response(
                            {"message": f"ã€{namespace}ã€‘ã€{deployment_name}ã€‘å‰©ä½™å¯è°ƒåº¦èŠ‚ç‚¹ä¸è¶³"},
                            status=500,
                        )
                else:
                    logger.info(f"å·²æœ‰{labeled_nodes_count}ä¸ªèŠ‚ç‚¹æœ‰æ ‡ç­¾ï¼Œæ— éœ€å†æ‰“æ ‡ç­¾")

            elif num < current_replicas and add_label == 'true':
                node_cpu_list = request_info[0].get("node_cpu_list")  # kubedoor-master scaleæ¥å£è¿½åŠ çš„å˜é‡
                logger.info(f"èŠ‚ç‚¹CPUæƒ…å†µ: {node_cpu_list}")
                logger.info(f"æ‰§è¡Œç¼©å®¹ï¼Œç›®æ ‡å‰¯æœ¬æ•°: {num}")
                del_label_count = current_replicas - num
                # é€‰æ‹©å½“å‰ CPU ä½¿ç”¨ç‡æœ€é«˜çš„èŠ‚ç‚¹ï¼Œç›´åˆ°æ»¡è¶³ç¼©å®¹åçš„å‰¯æœ¬æ•°
                available_nodes = await select_del_label_nodes(
                    namespace, del_label_count, deployment_name, node_cpu_list
                )
                for node in available_nodes:
                    await del_node_with_label(namespace, node, deployment_name)
                # åˆ é™¤available_nodesä¸­çš„pod
                await delete_pods_in_available_nodes(namespace, deployment_name, available_nodes)
                # åˆ é™¤podåç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè®©deploymentæ§åˆ¶å™¨å®Œæˆpodé‡å»º
                logger.info("ç­‰å¾…deploymentæ§åˆ¶å™¨å®Œæˆpodé‡å»º...")
                await asyncio.sleep(2)

            elif num == current_replicas:
                logger.info(f"å‰¯æœ¬æ•°æ²¡æœ‰å˜åŒ–ï¼Œæ— éœ€æ“ä½œ")
            else:
                logger.info(f"æ™®é€šæ¨¡å¼æ‰©ç¼©å®¹")

            # é‡è¯•æœºåˆ¶å¤„ç†409å†²çª
            max_retries = 3
            retry_count = 0
            if scheduler == 'true' and add_label == 'true':
                return web.json_response({"message": "add_labelå’Œschedulerå‚æ•°ä¸èƒ½åŒæ—¶ä¸ºTrue"}, status=400)

            # å¦‚æœå¯ç”¨schedulerï¼Œè§£æbodyè·å–node_scheduleråˆ—è¡¨
            if scheduler == 'true' and not job_name:
                try:
                    logger.info(
                        f"å¼€å§‹å¤„ç†schedulerå‚æ•°ï¼Œenv={utils.PROM_K8S_TAG_VALUE}, ns={namespace}, deployment={deployment_name}"
                    )

                    node_scheduler_list = request_info.get('node_scheduler', [])
                    logger.info(f"è·å–åˆ°node_scheduleråˆ—è¡¨: {node_scheduler_list}")

                    # ä½¿ç”¨å®¢æˆ·ç«¯ç®¡ç†å™¨ç¡®ä¿å®¢æˆ·ç«¯æ­£ç¡®å…³é—­
                    async with K8sClientManager() as k8s_manager:
                        logger.info(f"æˆåŠŸè·å–K8så®¢æˆ·ç«¯: {type(k8s_manager.core_v1_api)}")

                        # åˆå§‹åŒ–K8sèŠ‚ç‚¹è°ƒåº¦å™¨
                        logger.info("æ­£åœ¨åˆå§‹åŒ–K8sèŠ‚ç‚¹è°ƒåº¦å™¨...")
                        k8s_scheduler = K8sNodeScheduler(k8s_manager.core_v1_api)
                        logger.info(f"æˆåŠŸåˆå§‹åŒ–K8sèŠ‚ç‚¹è°ƒåº¦å™¨: {type(k8s_scheduler)}")

                        # æ‰§è¡Œç¦æ­¢è°ƒåº¦æ“ä½œ
                        logger.info(f"å¼€å§‹æ‰§è¡Œç¦æ­¢è°ƒåº¦æ“ä½œï¼Œæ’é™¤èŠ‚ç‚¹: {node_scheduler_list}")
                        cordon_result = await k8s_scheduler.cordon_nodes_exclude(exclude_nodes=node_scheduler_list)
                        logger.info(f"ç¦æ­¢è°ƒåº¦æ“ä½œå®Œæˆ: {cordon_result}")

                        # æ£€æŸ¥ cordon æ“ä½œæ˜¯å¦æœ‰é”™è¯¯
                        if cordon_result.get("error_count", 0) > 0:
                            error_details = []
                            for result in cordon_result.get("results", []):
                                if result.get("status") == "error":
                                    error_details.append(f"èŠ‚ç‚¹ {result.get('node_name')}: {result.get('message')}")

                            error_message = f"ç¦æ­¢èŠ‚ç‚¹è°ƒåº¦æ“ä½œå¤±è´¥ï¼Œé”™è¯¯è¯¦æƒ…: {'; '.join(error_details)}"
                            logger.error(error_message)

                            # æ‰§è¡Œæ¢å¤æ“ä½œï¼šå–æ¶ˆæ‰€æœ‰èŠ‚ç‚¹çš„ç¦æ­¢è°ƒåº¦çŠ¶æ€
                            try:
                                logger.warning("âš ï¸ cordonæ“ä½œå¤±è´¥ï¼Œå¼€å§‹æ‰§è¡Œuncordonæ¢å¤æ“ä½œä»¥ç¡®ä¿èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´æ€§...")
                                uncordon_result = await k8s_scheduler.uncordon_nodes_exclude(
                                    exclude_nodes=node_scheduler_list
                                )
                                logger.info(f"uncordonæ¢å¤æ“ä½œå®Œæˆ: {uncordon_result}")

                                if uncordon_result.get("error_count", 0) > 0:
                                    logger.error(f"âš ï¸ uncordonæ¢å¤æ“ä½œä¹Ÿå‡ºç°é”™è¯¯: {uncordon_result}")
                                    error_message += (
                                        f"ï¼›æ¢å¤æ“ä½œä¹Ÿå¤±è´¥: {uncordon_result.get('error_count', 0)}ä¸ªèŠ‚ç‚¹æ¢å¤å¤±è´¥"
                                    )
                                else:
                                    logger.info("âœ… uncordonæ¢å¤æ“ä½œæˆåŠŸï¼Œæ‰€æœ‰èŠ‚ç‚¹è°ƒåº¦çŠ¶æ€å·²æ¢å¤")
                                    error_message += "ï¼›å·²æ‰§è¡Œæ¢å¤æ“ä½œç¡®ä¿èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´æ€§"

                            except Exception as uncordon_e:
                                logger.error(
                                    f"âŒ æ‰§è¡Œuncordonæ¢å¤æ“ä½œæ—¶å‘ç”Ÿå¼‚å¸¸: {type(uncordon_e).__name__}: {str(uncordon_e)}"
                                )
                                error_message += f"ï¼›æ¢å¤æ“ä½œå¼‚å¸¸: {str(uncordon_e)}"

                            return web.json_response({"message": error_message}, status=500)

                except Exception as e:
                    logger.error(f"å¤„ç†schedulerå‚æ•°æ—¶å‘ç”Ÿå¼‚å¸¸: {type(e).__name__}: {str(e)}")
                    import traceback

                    logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")

                    # åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿæ‰§è¡Œæ¢å¤æ“ä½œ
                    if (
                        'node_scheduler_list' in locals()
                        and node_scheduler_list
                        and 'k8s_scheduler' in locals()
                        and k8s_scheduler
                    ):
                        try:
                            logger.warning(
                                "âš ï¸ å¤„ç†schedulerå‚æ•°æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œå¼€å§‹æ‰§è¡Œuncordonæ¢å¤æ“ä½œä»¥ç¡®ä¿èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´æ€§..."
                            )
                            uncordon_result = await k8s_scheduler.uncordon_nodes_exclude(
                                exclude_nodes=node_scheduler_list
                            )
                            logger.info(f"å¼‚å¸¸æƒ…å†µä¸‹çš„uncordonæ¢å¤æ“ä½œå®Œæˆ: {uncordon_result}")

                            if uncordon_result.get("error_count", 0) > 0:
                                logger.error(f"âš ï¸ å¼‚å¸¸æƒ…å†µä¸‹çš„uncordonæ¢å¤æ“ä½œä¹Ÿå‡ºç°é”™è¯¯: {uncordon_result}")
                            else:
                                logger.info("âœ… å¼‚å¸¸æƒ…å†µä¸‹çš„uncordonæ¢å¤æ“ä½œæˆåŠŸï¼Œæ‰€æœ‰èŠ‚ç‚¹è°ƒåº¦çŠ¶æ€å·²æ¢å¤")

                        except Exception as uncordon_e:
                            logger.error(
                                f"âŒ å¼‚å¸¸æƒ…å†µä¸‹æ‰§è¡Œuncordonæ¢å¤æ“ä½œæ—¶å‘ç”Ÿå¼‚å¸¸: {type(uncordon_e).__name__}: {str(uncordon_e)}"
                            )

                    return web.json_response({"message": f"å¤„ç†schedulerå‚æ•°å¤±è´¥: {str(e)}"}, status=500)

            while retry_count < max_retries:
                try:
                    # é‡æ–°è·å–deploymentå¯¹è±¡ï¼Œé¿å…resourceVersionå†²çª
                    if num < current_replicas and add_label == 'true':
                        deployment_obj = await v1.read_namespaced_deployment(deployment_name, namespace)
                    deployment_obj.spec.replicas = num
                    logger.info(
                        f"Deploymentã€{deployment_name}ã€‘å‰¯æœ¬æ•°æ›´æ”¹ä¸º {num}ï¼Œå¦‚å·²æ¥å…¥å‡†å…¥æ§åˆ¶, å®é™…å˜æ›´å·²æ•°æ®åº“ä¸­æ•°æ®ä¸ºå‡†ã€‚"
                    )

                    # å¦‚æœæ˜¯ä¸´æ—¶æ‰©ç¼©å®¹ï¼Œéœ€è¦ä½¿ç”¨å®Œæ•´çš„patchæ–¹æ³•æ¥ä¿å­˜annotations
                    if temp == 'true' or del_scale_temp == 1:
                        await v1.patch_namespaced_deployment(deployment_name, namespace, deployment_obj)
                    else:
                        await v1.patch_namespaced_deployment_scale(deployment_name, namespace, deployment_obj)
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                except ApiException as patch_e:
                    if patch_e.status == 409 and retry_count < max_retries - 1:
                        retry_count += 1
                        logger.warning(f"é‡åˆ°409å†²çªï¼Œç¬¬{retry_count}æ¬¡é‡è¯•...")
                        await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                    else:
                        raise patch_e  # é409é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°ç”¨å®Œï¼ŒæŠ›å‡ºå¼‚å¸¸

            if interval and index != len(request_info) - 1:
                logger.info(f"æš‚åœ {interval}s...")
                await asyncio.sleep(int(interval))

            utils.send_msg(
                f"'ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘' has been scaled! {current_replicas} --> {num}"
            )

            if job_name:
                await delete_cronjob_or_not(job_name, job_type)

            # å¦‚æœå¯ç”¨äº†schedulerï¼Œåœ¨æ ‡ç­¾ä¿®æ”¹å®Œæˆåæ‰§è¡Œå–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œï¼ˆå»¶è¿Ÿ10ç§’ï¼‰
            if scheduler == 'true' and not job_name:
                try:
                    logger.info(f"æ ‡ç­¾ä¿®æ”¹å®Œæˆï¼Œå‡†å¤‡æ‰§è¡Œå–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œï¼Œæ’é™¤èŠ‚ç‚¹: {node_scheduler_list}")
                    logger.info("æ­£åœ¨è°ƒç”¨uncordon_nodes_excludeæ–¹æ³•...")

                    # å®šä¹‰é”™è¯¯å›è°ƒå‡½æ•°
                    def uncordon_error_callback(error_message):
                        logger.error(f"å–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå¤±è´¥é€šçŸ¥: {error_message}")
                        utils.send_msg(
                            f"âš ï¸ å–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå¤±è´¥: {error_message}\n\n{utils.PROM_K8S_TAG_VALUE}, {namespace}, {deployment_name}"
                        )

                    # ä¸º uncordon æ“ä½œåˆ›å»ºæ–°çš„å®¢æˆ·ç«¯ç®¡ç†å™¨
                    async with K8sClientManager() as uncordon_k8s_manager:
                        uncordon_scheduler = K8sNodeScheduler(uncordon_k8s_manager.core_v1_api)
                        uncordon_result = await uncordon_scheduler.uncordon_nodes_exclude(
                            exclude_nodes=node_scheduler_list, delay_seconds=10, error_callback=uncordon_error_callback
                        )
                        logger.info(f"å–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå·²å®‰æ’: {uncordon_result}")
                except Exception as e:
                    logger.error(f"æ‰§è¡Œå–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå¤±è´¥: {type(e).__name__}: {str(e)}")
                    import traceback

                    logger.error(f"uncordonå¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                    # ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•é”™è¯¯

        except ApiException as e:
            logger.exception(f"è°ƒç”¨ AppsV1Api æ—¶å‡ºé”™: {e}")
            try:
                reason = json.loads(e.body).get("message", str(e))
            except:
                reason = str(e)
            error_list.append({'namespace': namespace, 'deployment_name': deployment_name, 'reason': reason})

    if error_list:
        return web.json_response({"message": f"ä»¥ä¸‹æœåŠ¡æœªæ‰©ç¼©å®¹æˆåŠŸ{error_list}", "success": False})
    else:
        return web.json_response({"message": "ok", "success": True})


async def reboot(request):
    """æ‰¹é‡é‡å¯å¾®æœåŠ¡"""
    request_info = await request.json()
    interval = request.query.get("interval")
    scheduler = request.query.get("scheduler", "false")
    patch = {
        "spec": {
            "template": {"metadata": {"annotations": {"kubectl.kubernetes.io/restartedAt": datetime.now().isoformat()}}}
        }
    }
    error_list = []

    for index, deployment in enumerate(request_info.get('deployment_list', [])):
        namespace = deployment.get("namespace")
        deployment_name = deployment.get("deployment_name")
        job_name = deployment.get("job_name")
        job_type = deployment.get("job_type")
        logger.info(f"ã€{namespace}ã€‘ã€{deployment_name}ã€‘")

        try:
            deployment_obj = await v1.read_namespaced_deployment(deployment_name, namespace)
            if not deployment_obj:
                reason = f"æœªæ‰¾åˆ°ã€{namespace}ã€‘ã€{deployment_name}ã€‘"
                logger.error(reason)
                error_list.append({'namespace': namespace, 'deployment_name': deployment_name, 'reason': reason})
                continue

            # å¦‚æœå¯ç”¨schedulerï¼Œè§£æbodyè·å–node_scheduleråˆ—è¡¨
            if scheduler == 'true' and not job_name:
                try:
                    logger.info(
                        f"å¼€å§‹å¤„ç†schedulerå‚æ•°ï¼Œenv={utils.PROM_K8S_TAG_VALUE}, ns={namespace}, deployment={deployment_name}"
                    )

                    node_scheduler_list = request_info.get('node_scheduler', [])
                    logger.info(f"è·å–åˆ°node_scheduleråˆ—è¡¨: {node_scheduler_list}")

                    # ä½¿ç”¨å®¢æˆ·ç«¯ç®¡ç†å™¨ç¡®ä¿å®¢æˆ·ç«¯æ­£ç¡®å…³é—­
                    async with K8sClientManager() as k8s_manager:
                        logger.info(f"æˆåŠŸè·å–K8så®¢æˆ·ç«¯: {type(k8s_manager.core_v1_api)}")

                        # åˆå§‹åŒ–K8sèŠ‚ç‚¹è°ƒåº¦å™¨
                        logger.info("æ­£åœ¨åˆå§‹åŒ–K8sèŠ‚ç‚¹è°ƒåº¦å™¨...")
                        k8s_scheduler = K8sNodeScheduler(k8s_manager.core_v1_api)
                        logger.info(f"æˆåŠŸåˆå§‹åŒ–K8sèŠ‚ç‚¹è°ƒåº¦å™¨: {type(k8s_scheduler)}")

                        # æ‰§è¡Œç¦æ­¢è°ƒåº¦æ“ä½œ
                        logger.info(f"å¼€å§‹æ‰§è¡Œç¦æ­¢è°ƒåº¦æ“ä½œï¼Œæ’é™¤èŠ‚ç‚¹: {node_scheduler_list}")
                        cordon_result = await k8s_scheduler.cordon_nodes_exclude(exclude_nodes=node_scheduler_list)
                        logger.info(f"ç¦æ­¢è°ƒåº¦æ“ä½œå®Œæˆ: {cordon_result}")

                        # æ£€æŸ¥ cordon æ“ä½œæ˜¯å¦æœ‰é”™è¯¯
                        if cordon_result.get("error_count", 0) > 0:
                            error_details = []
                            for result in cordon_result.get("results", []):
                                if result.get("status") == "error":
                                    error_details.append(f"èŠ‚ç‚¹ {result.get('node_name')}: {result.get('message')}")

                            error_message = f"ç¦æ­¢èŠ‚ç‚¹è°ƒåº¦æ“ä½œå¤±è´¥ï¼Œé”™è¯¯è¯¦æƒ…: {'; '.join(error_details)}"
                            logger.error(error_message)

                            # æ‰§è¡Œæ¢å¤æ“ä½œï¼šå–æ¶ˆæ‰€æœ‰èŠ‚ç‚¹çš„ç¦æ­¢è°ƒåº¦çŠ¶æ€
                            try:
                                logger.warning("âš ï¸ cordonæ“ä½œå¤±è´¥ï¼Œå¼€å§‹æ‰§è¡Œuncordonæ¢å¤æ“ä½œä»¥ç¡®ä¿èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´æ€§...")
                                uncordon_result = await k8s_scheduler.uncordon_nodes_exclude(
                                    exclude_nodes=node_scheduler_list
                                )
                                logger.info(f"uncordonæ¢å¤æ“ä½œå®Œæˆ: {uncordon_result}")

                                if uncordon_result.get("error_count", 0) > 0:
                                    logger.error(f"âš ï¸ uncordonæ¢å¤æ“ä½œä¹Ÿå‡ºç°é”™è¯¯: {uncordon_result}")
                                    error_message += (
                                        f"ï¼›æ¢å¤æ“ä½œä¹Ÿå¤±è´¥: {uncordon_result.get('error_count', 0)}ä¸ªèŠ‚ç‚¹æ¢å¤å¤±è´¥"
                                    )
                                else:
                                    logger.info("âœ… uncordonæ¢å¤æ“ä½œæˆåŠŸï¼Œæ‰€æœ‰èŠ‚ç‚¹è°ƒåº¦çŠ¶æ€å·²æ¢å¤")
                                    error_message += "ï¼›å·²æ‰§è¡Œæ¢å¤æ“ä½œç¡®ä¿èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´æ€§"

                            except Exception as uncordon_e:
                                logger.error(
                                    f"âŒ æ‰§è¡Œuncordonæ¢å¤æ“ä½œæ—¶å‘ç”Ÿå¼‚å¸¸: {type(uncordon_e).__name__}: {str(uncordon_e)}"
                                )
                                error_message += f"ï¼›æ¢å¤æ“ä½œå¼‚å¸¸: {str(uncordon_e)}"

                            return web.json_response({"message": error_message}, status=500)

                except Exception as e:
                    logger.error(f"å¤„ç†schedulerå‚æ•°æ—¶å‘ç”Ÿå¼‚å¸¸: {type(e).__name__}: {str(e)}")
                    import traceback

                    logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")

                    # åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿæ‰§è¡Œæ¢å¤æ“ä½œ
                    if (
                        'node_scheduler_list' in locals()
                        and node_scheduler_list
                        and 'k8s_scheduler' in locals()
                        and k8s_scheduler
                    ):
                        try:
                            logger.warning(
                                "âš ï¸ å¤„ç†schedulerå‚æ•°æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œå¼€å§‹æ‰§è¡Œuncordonæ¢å¤æ“ä½œä»¥ç¡®ä¿èŠ‚ç‚¹çŠ¶æ€ä¸€è‡´æ€§..."
                            )
                            uncordon_result = await k8s_scheduler.uncordon_nodes_exclude(
                                exclude_nodes=node_scheduler_list
                            )
                            logger.info(f"å¼‚å¸¸æƒ…å†µä¸‹çš„uncordonæ¢å¤æ“ä½œå®Œæˆ: {uncordon_result}")

                            if uncordon_result.get("error_count", 0) > 0:
                                logger.error(f"âš ï¸ å¼‚å¸¸æƒ…å†µä¸‹çš„uncordonæ¢å¤æ“ä½œä¹Ÿå‡ºç°é”™è¯¯: {uncordon_result}")
                            else:
                                logger.info("âœ… å¼‚å¸¸æƒ…å†µä¸‹çš„uncordonæ¢å¤æ“ä½œæˆåŠŸï¼Œæ‰€æœ‰èŠ‚ç‚¹è°ƒåº¦çŠ¶æ€å·²æ¢å¤")

                        except Exception as uncordon_e:
                            logger.error(
                                f"âŒ å¼‚å¸¸æƒ…å†µä¸‹æ‰§è¡Œuncordonæ¢å¤æ“ä½œæ—¶å‘ç”Ÿå¼‚å¸¸: {type(uncordon_e).__name__}: {str(uncordon_e)}"
                            )

                    return web.json_response({"message": f"å¤„ç†schedulerå‚æ•°å¤±è´¥: {str(e)}"}, status=500)

            logger.info(f"é‡å¯ Deploymentã€{deployment_name}ã€‘ï¼Œå¦‚å·²æ¥å…¥å‡†å…¥æ§åˆ¶, å®é™…å˜æ›´å·²æ•°æ®åº“ä¸­æ•°æ®ä¸ºå‡†ã€‚")
            await v1.patch_namespaced_deployment(deployment_name, namespace, patch)

            if interval and index != len(request_info) - 1:
                logger.info(f"æš‚åœ {interval}s...")
                await asyncio.sleep(int(interval))

            utils.send_msg(f"'ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘' has been restarted!")

            if job_name:
                await delete_cronjob_or_not(job_name, job_type)

            # å¦‚æœå¯ç”¨äº†schedulerï¼Œåœ¨æ ‡ç­¾ä¿®æ”¹å®Œæˆåæ‰§è¡Œå–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œï¼ˆå»¶è¿Ÿ10ç§’ï¼‰
            if scheduler == 'true' and not job_name:
                try:
                    logger.info(f"æ ‡ç­¾ä¿®æ”¹å®Œæˆï¼Œå‡†å¤‡æ‰§è¡Œå–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œï¼Œæ’é™¤èŠ‚ç‚¹: {node_scheduler_list}")
                    logger.info("æ­£åœ¨è°ƒç”¨uncordon_nodes_excludeæ–¹æ³•...")

                    # å®šä¹‰é”™è¯¯å›è°ƒå‡½æ•°
                    def uncordon_error_callback(error_message):
                        logger.error(f"å–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå¤±è´¥é€šçŸ¥: {error_message}")
                        utils.send_msg(
                            f"âš ï¸ å–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå¤±è´¥: {error_message}\n\n{utils.PROM_K8S_TAG_VALUE}, {namespace}, {deployment_name}"
                        )

                    # ä¸º uncordon æ“ä½œåˆ›å»ºæ–°çš„å®¢æˆ·ç«¯ç®¡ç†å™¨
                    async with K8sClientManager() as uncordon_k8s_manager:
                        uncordon_scheduler = K8sNodeScheduler(uncordon_k8s_manager.core_v1_api)
                        uncordon_result = await uncordon_scheduler.uncordon_nodes_exclude(
                            exclude_nodes=node_scheduler_list, delay_seconds=120, error_callback=uncordon_error_callback
                        )
                        logger.info(f"å–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå·²å®‰æ’: {uncordon_result}")
                except Exception as e:
                    logger.error(f"æ‰§è¡Œå–æ¶ˆç¦æ­¢è°ƒåº¦æ“ä½œå¤±è´¥: {type(e).__name__}: {str(e)}")
                    import traceback

                    logger.error(f"uncordonå¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                    # ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•é”™è¯¯

        except ApiException as e:
            logger.exception(f"è°ƒç”¨ AppsV1Api æ—¶å‡ºé”™: {e}")
            try:
                reason = json.loads(e.body).get("message", str(e))
            except:
                reason = str(e)
            error_list.append({'namespace': namespace, 'deployment_name': deployment_name, 'reason': reason})

    return web.json_response({"message": "ok", "error_list": error_list})


async def cron(request):
    """åˆ›å»ºå®šæ—¶ä»»åŠ¡ï¼Œæ‰§è¡Œæ‰©ç¼©å®¹æˆ–é‡å¯"""
    request_info = await request.json()
    cron_expr = request_info.get("cron")
    time_expr = request_info.get("time")
    type_expr = request_info.get("type")
    service = request_info.get("service")
    add_label = request.query.get("add_label")

    deployment_name = service[0].get("deployment_name")
    name_pre = f"{type_expr}-{'once' if time_expr else 'cron'}-{deployment_name}"
    job_type = "once" if time_expr else "cron"
    cron_new = f"{time_expr[4]} {time_expr[3]} {time_expr[2]} {time_expr[1]} *" if time_expr else cron_expr
    service[0]["job_name"] = name_pre
    service[0]["job_type"] = job_type

    if add_label:
        url = f"https://kubedoor-agent.kubedoor/api/{type_expr}?add_label={add_label}"
    else:
        url = f"https://kubedoor-agent.kubedoor/api/{type_expr}"

    cronjob = client.V1CronJob(
        metadata=client.V1ObjectMeta(name=name_pre),
        spec=client.V1CronJobSpec(
            schedule=cron_new,
            job_template=client.V1JobTemplateSpec(
                spec=client.V1JobSpec(
                    template=client.V1PodTemplateSpec(
                        metadata=client.V1ObjectMeta(labels={"app": name_pre}),
                        spec=client.V1PodSpec(
                            restart_policy="Never",
                            containers=[
                                client.V1Container(
                                    name=name_pre,
                                    image="registry.cn-shenzhen.aliyuncs.com/starsl/busybox-curl",
                                    command=[
                                        "curl",
                                        "-s",
                                        "-k",
                                        "-X",
                                        "POST",
                                        "-H",
                                        "Content-Type: application/json",
                                        "-d",
                                        f'{json.dumps(service)}',
                                        url,
                                    ],
                                    env=[client.V1EnvVar(name="CRONJOB_TYPE", value=job_type)],
                                )
                            ],
                        ),
                    )
                )
            ),
        ),
    )

    namespace = "kubedoor"
    try:
        await batch_v1.create_namespaced_cron_job(namespace=namespace, body=cronjob)
        content = f"CronJob '{name_pre}' created successfully."
        logger.info(content)
        utils.send_msg(f'ã€{utils.PROM_K8S_TAG_VALUE}ã€‘{content}')
        return web.json_response({"message": "ok"})
    except Exception as e:
        error_message = json.loads(e.body).get("message") if hasattr(e, 'body') and e.body else "æ‰§è¡Œå¤±è´¥"
        logger.error(error_message)
        return web.json_response({"message": error_message}, status=500)


async def update_namespace_label(ns, action):
    """å‘½åç©ºé—´ä¿®æ”¹æ ‡ç­¾"""
    namespace_name = ns
    label_key = "kubedoor-ignore"
    label_value = 'true' if action == "add" else None
    patch_body = {"metadata": {"labels": {label_key: label_value}}}

    try:
        response = await core_v1.patch_namespace(name=namespace_name, body=patch_body)
        logger.info(
            f"Label '{label_key}: {label_value}' {'added to' if action == 'add' else 'removed from'} namespace '{namespace_name}' successfully."
        )
        logger.info(f"Updated namespace labels: {response.metadata.labels}")
    except ApiException as e:
        logger.error(f"Exception when patching namespace '{namespace_name}': {e}")


async def get_mutating_webhook():
    """è·å– MutatingWebhookConfiguration"""
    webhook_name = "kubedoor-admis-configuration"
    try:
        await admission_api.read_mutating_webhook_configuration(name=webhook_name)
        logger.info(f"MutatingWebhookConfiguration '{webhook_name}' exists.")
        return {"is_on": True}
    except ApiException as e:
        if e.status == 404:
            logger.error(f"MutatingWebhookConfiguration '{webhook_name}' does not exist.")
            return {"is_on": False}
        error_message = json.loads(e.body).get("message") if hasattr(e, 'body') and e.body else "æ‰§è¡Œå¤±è´¥"
        logger.error(f"Exception when reading MutatingWebhookConfiguration: {e}")
        return {"message": error_message}


async def create_mutating_webhook():
    """åˆ›å»º MutatingWebhookConfiguration"""
    webhook_name = "kubedoor-admis-configuration"
    namespace = "kubedoor"
    webhook_config = client.V1MutatingWebhookConfiguration(
        metadata=client.V1ObjectMeta(name=webhook_name),
        webhooks=[
            client.V1MutatingWebhook(
                name="kubedoor-admis.mutating.webhook",
                client_config=client.AdmissionregistrationV1WebhookClientConfig(
                    service=client.AdmissionregistrationV1ServiceReference(
                        namespace=namespace, name="kubedoor-agent", path="/api/admis", port=443
                    ),
                    ca_bundle=utils.BASE64CA,
                ),
                rules=[
                    client.V1RuleWithOperations(
                        operations=["CREATE", "UPDATE"],
                        api_groups=["apps"],
                        api_versions=["v1"],
                        resources=["deployments", "deployments/scale"],
                        scope="*",
                    )
                ],
                failure_policy="Fail",
                match_policy="Equivalent",
                namespace_selector=client.V1LabelSelector(
                    match_expressions=[
                        client.V1LabelSelectorRequirement(key="kubedoor-ignore", operator="DoesNotExist")
                    ]
                ),
                side_effects="None",
                timeout_seconds=30,
                admission_review_versions=["v1"],
                reinvocation_policy="Never",
            )
        ],
    )

    try:
        response = await admission_api.create_mutating_webhook_configuration(body=webhook_config)
        logger.info(f"MutatingWebhookConfiguration created: {response.metadata.name}")
        await update_namespace_label("kube-system", "add")
        await update_namespace_label("kubedoor", "add")
    except ApiException as e:
        error_message = json.loads(e.body).get("message") if hasattr(e, 'body') and e.body else "æ‰§è¡Œå¤±è´¥"
        logger.error(f"Exception when creating MutatingWebhookConfiguration: {e}")
        return {"message": error_message}


async def delete_mutating_webhook():
    """åˆ é™¤ MutatingWebhookConfiguration"""
    webhook_name = "kubedoor-admis-configuration"
    try:
        await admission_api.delete_mutating_webhook_configuration(name=webhook_name)
        logger.info(f"MutatingWebhookConfiguration '{webhook_name}' deleted successfully.")
        await update_namespace_label("kube-system", "delete")
        await update_namespace_label("kubedoor", "delete")
    except ApiException as e:
        error_message = json.loads(e.body).get("message") if hasattr(e, 'body') and e.body else "æ‰§è¡Œå¤±è´¥"
        logger.error(f"Exception when deleting MutatingWebhookConfiguration: {e}")
        return {"message": error_message}


async def admis_switch(request):
    action = request.query.get("action")
    res = await get_mutating_webhook()

    if action == "get":
        return web.json_response(res)
    elif action == "on":
        if res.get("is_on"):
            return web.json_response({"message": "Webhook is already opened!", "success": True})
        create_res = await create_mutating_webhook()
        if create_res:
            return web.json_response(create_res, status=500)
    elif action == "off":
        if not res.get("is_on"):
            return web.json_response({"message": "Webhook is already closed!", "success": True})
        delete_res = await delete_mutating_webhook()
        if delete_res:
            return web.json_response(delete_res, status=500)

    return web.json_response({"message": "æ‰§è¡ŒæˆåŠŸ", "success": True})


async def get_namespace_events(request):
    """è·å–æŒ‡å®šå‘½åç©ºé—´çš„äº‹ä»¶ï¼Œå¦‚æœä¸æŒ‡å®šnamespaceåˆ™è·å–æ‰€æœ‰å‘½åç©ºé—´çš„äº‹ä»¶"""
    namespace = request.query.get("namespace")

    try:
        # æ„é€ æŸ¥è¯¢æ¡ä»¶
        field_selector = None
        if namespace:
            field_selector = f"involvedObject.namespace={namespace}"
            logger.info(f"è·å–å‘½åç©ºé—´ {namespace} çš„äº‹ä»¶")
        else:
            logger.info("è·å–æ‰€æœ‰å‘½åç©ºé—´çš„äº‹ä»¶")

        # è·å–äº‹ä»¶
        events = await core_v1.list_event_for_all_namespaces(field_selector=field_selector, _request_timeout=30)

        # æ ¼å¼åŒ–äº‹ä»¶æ•°æ®
        event_list = []
        for event in events.items:
            event_list.append(
                {
                    "name": event.metadata.name,
                    "namespace": event.metadata.namespace,
                    "type": event.type,
                    "reason": event.reason,
                    "message": event.message,
                    "involved_object": {
                        "kind": event.involved_object.kind,
                        "name": event.involved_object.name,
                        "namespace": event.involved_object.namespace,
                    },
                    "count": event.count,
                    "first_timestamp": (event.first_timestamp.isoformat() if event.first_timestamp else None),
                    "last_timestamp": (event.last_timestamp.isoformat() if event.last_timestamp else None),
                    "source": (
                        {"component": event.source.component, "host": event.source.host} if event.source else None
                    ),
                }
            )

        logger.info(f"è·å–äº‹ä»¶æˆåŠŸï¼Œå…± {len(event_list)} æ¡")
        return web.json_response({"events": event_list, "success": True})
    except ApiException as e:
        error_message = f"è·å–äº‹ä»¶å¤±è´¥: {e}"
        logger.error(error_message)
        return web.json_response({"message": error_message, "success": False}, status=500)
    except Exception as e:
        error_message = f"è·å–äº‹ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"
        logger.exception(error_message)
        return web.json_response({"message": error_message, "success": False}, status=500)


async def get_pod_metrics(namespace, pod_name):
    """è·å–æŒ‡å®šPodçš„CPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        # ä½¿ç”¨Metrics APIè·å–Podçš„èµ„æºä½¿ç”¨æƒ…å†µ
        metrics = await custom_api.get_namespaced_custom_object(
            group="metrics.k8s.io",
            version="v1beta1",
            namespace=namespace,
            plural="pods",
            name=pod_name,
        )

        # åˆå§‹åŒ–è¿”å›ç»“æ„
        cpu_usage = 0
        memory_usage = 0

        # ç´¯è®¡æ‰€æœ‰å®¹å™¨çš„èµ„æºä½¿ç”¨
        for container in metrics.get("containers", []):
            cpu = container.get("usage", {}).get("cpu", "0")
            memory = container.get("usage", {}).get("memory", "0")

            cpu = utils.parse_cpu(cpu)
            memory = utils.parse_memory(memory)

            cpu_usage += cpu
            memory_usage += memory

        return {
            "cpu": round(cpu_usage, 2),
            "memory": round(memory_usage, 2),
        }  # å•ä½ï¼šmCPU (æ¯«æ ¸)  # å•ä½ï¼šMB
    except Exception as e:
        logger.error(f"è·å–Pod {pod_name} èµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
        return {"cpu": 0, "memory": 0}


async def get_pod_events(namespace, pod_name):
    """è·å–æŒ‡å®šPodçš„äº‹ä»¶ä¿¡æ¯"""
    try:
        # ä½¿ç”¨field_selectorè¿‡æ»¤ç‰¹å®šPodçš„äº‹ä»¶
        field_selector = f"involvedObject.kind=Pod,involvedObject.name={pod_name}"
        events = await core_v1.list_namespaced_event(namespace=namespace, field_selector=field_selector)

        # æŒ‰æ—¶é—´é™åºæ’åºï¼Œè·å–æœ€è¿‘çš„äº‹ä»¶
        sorted_events = sorted(
            events.items,
            key=lambda event: event.last_timestamp or event.first_timestamp or event.metadata.creation_timestamp,
            reverse=True,
        )

        # è¿”å›æœ€æ–°çš„äº‹ä»¶æ¶ˆæ¯
        if sorted_events:
            latest_event = sorted_events[0]
            return latest_event.reason, latest_event.message

        return "", ""
    except Exception as e:
        logger.error(f"è·å–Pod {pod_name} äº‹ä»¶å¤±è´¥: {e}")
        return "", ""


async def get_deployment_pods(request):
    """è·å–æŒ‡å®šå‘½åç©ºé—´å’ŒDeploymentä¸‹çš„æ‰€æœ‰Podä¿¡æ¯ï¼ˆåŒ…æ‹¬è¢«éš”ç¦»çš„Podï¼‰"""
    namespace = request.query.get("namespace")
    deployment_name = request.query.get("deployment")

    try:
        # è·å–æŒ‡å®šDeploymentçš„æ ‡ç­¾é€‰æ‹©å™¨
        deployment = await v1.read_namespaced_deployment(deployment_name, namespace)
        selector = deployment.spec.selector.match_labels
        selector_str = ",".join([f"{k}={v}" for k, v in selector.items()])
        # 1. é€šè¿‡æ ‡ç­¾é€‰æ‹©å™¨æŸ¥è¯¢ç›¸å…³çš„Pod
        pods_by_label = await core_v1.list_namespaced_pod(namespace=namespace, label_selector=selector_str)
        lenmline = pods_by_label.items[0].metadata.name.count("-")
        # 2. é€šè¿‡ ownerReferences æŸ¥è¯¢æ‰€æœ‰å±äºè¯¥ deployment çš„ podï¼ˆå³ä½¿ label è¢«ä¿®æ”¹ä¹Ÿèƒ½æŸ¥åˆ°ï¼‰
        all_pods = await core_v1.list_namespaced_pod(namespace=namespace)
        pods_by_match = []
        for pod in all_pods.items:
            owner_refs = pod.metadata.owner_references or []
            # æ™ºèƒ½åŒ¹é…ï¼šå¦‚æœæ²¡æœ‰ownerReferencesï¼Œå°è¯•ç”¨podåç§°å‰ç¼€ã€é•œåƒç­‰ç‰¹å¾åˆ¤æ–­
            if (
                not owner_refs
                and pod.metadata.name.startswith(deployment_name + '-')
                and pod.metadata.name.count("-") == lenmline
            ):
                pods_by_match.append(pod)
                # é•œåƒåŒ¹é…ï¼ˆå¯æ ¹æ®å®é™…éœ€æ±‚æ‰©å±•æ›´å¤æ‚çš„è§„åˆ™ï¼‰
                # deployment_images = [c.image for c in deployment.spec.template.spec.containers]
                # pod_images = [c.image for c in pod.spec.containers]
                # if any(img in deployment_images for img in pod_images):

        # åˆå¹¶ podï¼Œå»é‡
        all_related_pods = {
            pod.metadata.name: pod
            for pod in pods_by_label.items
            if pod.metadata.name.startswith(deployment_name + '-') and pod.metadata.name.count("-") == lenmline
        }
        for pod in pods_by_match:
            all_related_pods[pod.metadata.name] = pod

        # æ„å»ºPodä¿¡æ¯åˆ—è¡¨
        pod_list = []
        for pod in all_related_pods.values():
            # è·å–Podèµ„æºä½¿ç”¨æƒ…å†µ
            metrics = await get_pod_metrics(namespace, pod.metadata.name)

            # å¤„ç†created_atä¸ºåŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–
            created_at = None
            if pod.metadata.creation_timestamp:
                utc_time = pod.metadata.creation_timestamp.replace(tzinfo=timezone.utc)
                beijing_time = utc_time.astimezone(timezone(timedelta(hours=8)))
                created_at = beijing_time.strftime("%Y-%m-%d %H:%M:%S")

            # å¤„ç†CPUå’Œå†…å­˜ä¸ºæ•´æ•°
            cpu = round(metrics["cpu"])
            memory = round(metrics["memory"])

            # è·å–Podè¯¦ç»†çŠ¶æ€åŸå› 
            pod_status_reason = ""

            # å¯¹éRunningçŠ¶æ€çš„Podè·å–æ›´è¯¦ç»†çš„åŸå› 
            if pod.status.phase != "Running":
                # 1. ä»PodçŠ¶æ€æœ¬èº«è·å–åŸå› 
                if pod.status.conditions:
                    for cond in pod.status.conditions:
                        if cond.type == "PodScheduled" and cond.status != "True":
                            pod_status_reason = cond.message or cond.reason or ""
                            break
                if not pod_status_reason and pod.status.reason:
                    pod_status_reason = pod.status.reason

                # 2. å¯¹äºæ‰€æœ‰éRunningçš„Podï¼Œä»å®¹å™¨çŠ¶æ€è·å–åŸå› 
                if not pod_status_reason and pod.status.container_statuses:
                    for cs in pod.status.container_statuses:
                        if cs.state and (cs.state.waiting or cs.state.terminated):
                            if cs.state.waiting:
                                container_reason = cs.state.waiting.reason or ""
                                container_message = cs.state.waiting.message or ""
                                pod_status_reason = (
                                    f"{container_reason}: {container_message}"
                                    if container_message
                                    else container_reason
                                )
                            elif cs.state.terminated:
                                container_reason = cs.state.terminated.reason or ""
                                container_message = cs.state.terminated.message or ""
                                exit_code = cs.state.terminated.exit_code
                                pod_status_reason = (
                                    f"{container_reason} (exit: {exit_code}): {container_message}"
                                    if container_message
                                    else f"{container_reason} (exit: {exit_code})"
                                )
                            break

                # 3. å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æ²¡æœ‰è·å–åˆ°åŸå› ï¼Œå°è¯•ä»æœ€æ–°äº‹ä»¶è·å–
                if not pod_status_reason:
                    event_reason, event_message = await get_pod_events(namespace, pod.metadata.name)
                    if event_message:
                        pod_status_reason = f"{event_reason}: {event_message}" if event_reason else event_message

            # è·å–Last Statusï¼Œåªåœ¨Podæœ‰é‡å¯æ—¶æ‰è·å–
            last_status = ""
            restart_count = (
                sum(container_status.restart_count for container_status in pod.status.container_statuses)
                if pod.status.container_statuses
                else 0
            )
            if restart_count > 0 and pod.status.container_statuses:
                for cs in pod.status.container_statuses:
                    if cs.last_state and (cs.last_state.terminated or cs.last_state.waiting):
                        if cs.last_state.terminated:
                            last_status = f"Terminated: {cs.last_state.terminated.reason or ''} ({cs.last_state.terminated.exit_code})"
                        elif cs.last_state.waiting:
                            last_status = f"Waiting: {cs.last_state.waiting.reason or ''}"
                        break

            # è·å–ä¸»å®¹å™¨é•œåƒä¿¡æ¯
            main_container_image = ""
            if pod.spec.containers and len(pod.spec.containers) > 0:
                main_container_image = pod.spec.containers[0].image

            pod_info = {
                "name": pod.metadata.name,
                "status": pod.status.phase,
                "ready": (
                    all(container_status.ready for container_status in pod.status.container_statuses)
                    if pod.status.container_statuses
                    else False
                ),
                "pod_ip": pod.status.pod_ip,
                "cpu": f"{cpu}m",
                "memory": f"{memory}MB",
                "created_at": created_at,
                "app_label": pod.metadata.labels.get("app", "æ— "),
                "image": main_container_image,
                "node_name": pod.spec.node_name,
                "restart_count": restart_count,
                "restart_reason": last_status,
                "exception_reason": pod_status_reason,  # æ˜¾ç¤ºæ‰€æœ‰éRunningçŠ¶æ€çš„åŸå› 
            }
            pod_list.append(pod_info)

        return web.json_response(
            {
                "success": True,
                "pods": pod_list,
            }
        )
    except ApiException as e:
        error_message = (
            json.loads(e.body).get("message") if hasattr(e, 'body') and e.body else f"è·å–Podä¿¡æ¯å¤±è´¥: {str(e)}"
        )
        logger.error(error_message)
        return web.json_response({"message": error_message, "success": False}, status=500)
    except Exception as e:
        error_message = f"è·å–Podä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"
        logger.exception(error_message)
        return web.json_response({"message": error_message, "success": False}, status=500)


async def get_node_metrics(node_name):
    """è·å–èŠ‚ç‚¹çš„èµ„æºä½¿ç”¨æƒ…å†µ"""
    try:
        # ä½¿ç”¨Metrics APIè·å–èŠ‚ç‚¹çš„èµ„æºä½¿ç”¨æƒ…å†µ
        metrics = await custom_api.get_cluster_custom_object(
            group="metrics.k8s.io", version="v1beta1", plural="nodes", name=node_name
        )

        # åˆå§‹åŒ–CPUå’Œå†…å­˜ä½¿ç”¨
        cpu_usage = 0
        memory_usage = 0

        # è·å–ä½¿ç”¨é‡
        if metrics and "usage" in metrics:
            cpu = metrics["usage"].get("cpu", "0")
            memory = metrics["usage"].get("memory", "0")

            # è½¬æ¢CPUå€¼ï¼ˆä»å­—ç¬¦ä¸²å¦‚ "100m" è½¬ä¸ºæ•°å€¼ 100ï¼‰
            if isinstance(cpu, str):
                cpu_usage = utils.parse_cpu(cpu)
            else:
                cpu_usage = utils.parse_cpu(str(cpu))

            if isinstance(memory, str):
                memory_usage = utils.parse_memory(memory)
            else:
                memory_usage = utils.parse_memory(str(memory))

        return {
            "cpu": round(cpu_usage, 2),
            "memory": round(memory_usage, 2),
        }  # å•ä½ï¼šmCPU (æ¯«æ ¸)å’ŒMB
    except Exception as e:
        logger.error(f"è·å–èŠ‚ç‚¹ {node_name} èµ„æºä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
        return {"cpu": 0, "memory": 0}


async def get_nodes_info(request):
    """è·å–æ‰€æœ‰K8SèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        logger.info("å¼€å§‹è·å–K8SèŠ‚ç‚¹ä¿¡æ¯...")

        # è·å–æ‰€æœ‰èŠ‚ç‚¹åˆ—è¡¨
        nodes = await core_v1.list_node()

        # è·å–æ‰€æœ‰Podåˆ—è¡¨ï¼ˆç”¨äºè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„Podæ•°é‡ï¼‰
        pods = await core_v1.list_pod_for_all_namespaces()

        # åˆå§‹åŒ–è¿”å›ç»“æœ
        node_list = []

        for node in nodes.items:
            node_name = node.metadata.name

            # è·å–èŠ‚ç‚¹çš„IPåœ°å€
            node_ip = ""
            for address in node.status.addresses:
                if address.type == "InternalIP":
                    node_ip = address.address
                    break

            # è·å–èŠ‚ç‚¹çš„ç³»ç»Ÿä¿¡æ¯
            container_runtime = node.status.node_info.container_runtime_version
            os_image = f"{node.status.node_info.os_image} {node.status.node_info.kernel_version}"
            kubelet_version = node.status.node_info.kubelet_version

            # è·å–èŠ‚ç‚¹çš„çŠ¶æ€æ¡ä»¶ï¼Œåªä¿ç•™statusä¸ºTrueçš„
            conditions = []
            for condition in node.status.conditions:
                if condition.status == "True":
                    conditions.append(condition.type)

            # è·å–èŠ‚ç‚¹çš„å¯åˆ†é…èµ„æº
            allocatable_cpu = 0
            allocatable_memory = 0
            max_pods = 0

            if node.status.allocatable:
                # CPUå•ä½é€šå¸¸æ˜¯æ ¸å¿ƒæ•°ï¼Œå¦‚"4"è¡¨ç¤º4æ ¸å¿ƒ
                allocatable_cpu_str = node.status.allocatable.get("cpu", "0")
                allocatable_cpu = utils.parse_cpu(allocatable_cpu_str)

                # å†…å­˜å•ä½é€šå¸¸æ˜¯å­—èŠ‚ï¼Œéœ€è¦è½¬æ¢ä¸ºMB
                allocatable_memory_str = node.status.allocatable.get("memory", "0")
                allocatable_memory = utils.parse_memory(allocatable_memory_str)

                # æœ€å¤§å¯è¿è¡ŒPodæ•°
                max_pods_str = node.status.allocatable.get("pods", "0")
                try:
                    max_pods = int(max_pods_str)
                except (ValueError, AttributeError):
                    max_pods = 0

            # è®¡ç®—å½“å‰èŠ‚ç‚¹ä¸Šè¿è¡Œçš„Podæ•°é‡
            current_pods = 0
            for pod in pods.items:
                if pod.spec.node_name == node_name:
                    current_pods += 1

            # è·å–èŠ‚ç‚¹å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
            metrics = await get_node_metrics(node_name)
            current_cpu = metrics["cpu"]  # mCPU
            current_memory = metrics["memory"]  # MB

            # æ„å»ºèŠ‚ç‚¹ä¿¡æ¯
            node_info = {
                "name": node_name,
                "ip": node_ip,
                "os_image": os_image,
                "container_runtime": container_runtime,
                "kubelet_version": kubelet_version,
                "conditions": ", ".join(conditions) if conditions else "",
                "allocatable_cpu": round(allocatable_cpu),
                "current_cpu": round(current_cpu),
                "allocatable_memory": round(allocatable_memory),
                "current_memory": round(current_memory),
                "max_pods": max_pods,
                "current_pods": current_pods,
            }

            node_list.append(node_info)

        logger.info(f"è·å–èŠ‚ç‚¹ä¿¡æ¯æˆåŠŸï¼Œå…± {len(node_list)} ä¸ªèŠ‚ç‚¹")
        return web.json_response({"nodes": node_list, "success": True})

    except ApiException as e:
        error_message = f"è·å–èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {e}"
        logger.error(error_message)
        return web.json_response({"message": error_message, "success": False}, status=500)
    except Exception as e:
        error_message = f"è·å–èŠ‚ç‚¹ä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"
        logger.exception(error_message)
        return web.json_response({"message": error_message, "success": False}, status=500)


async def balance_node(request):
    """èŠ‚ç‚¹å¾®è°ƒå‡è¡¡ - å°†éƒ¨ç½²ä»æºèŠ‚ç‚¹è¿ç§»åˆ°ç›®æ ‡èŠ‚ç‚¹"""
    try:
        data = await request.json()
        env = data.get("env")
        source_node = data.get("source")
        target_node = data.get("target")
        top_deployments = data.get("top_deployments", [])

        if not source_node or not target_node or not top_deployments:
            return web.json_response({"message": "ç¼ºå°‘å¿…è¦å‚æ•°", "success": False}, status=400)

        logger.info(f"å¼€å§‹èŠ‚ç‚¹å‡è¡¡: æºèŠ‚ç‚¹ {source_node} -> ç›®æ ‡èŠ‚ç‚¹ {target_node}")
        logger.info(f"å¾…è¿ç§»çš„deployment: {json.dumps(top_deployments)}")

        # å­˜å‚¨æ“ä½œç»“æœ
        results = []

        for deployment_info in top_deployments:
            namespace = deployment_info.get("namespace")
            deployment_name = deployment_info.get("deployment")

            if not namespace or not deployment_name:
                continue

            try:
                # 1. æ„é€ æ ‡ç­¾é”®
                label_key = f"{namespace}.{deployment_name}"
                logger.info(f"å¤„ç†æ ‡ç­¾: {label_key}={utils.NODE_LABLE_VALUE}")

                # 2. ä»æºèŠ‚ç‚¹åˆ é™¤æ ‡ç­¾
                await remove_node_label(source_node, label_key)

                # 3. åœ¨ç›®æ ‡èŠ‚ç‚¹æ·»åŠ æ ‡ç­¾
                await update_node_with_label(namespace, target_node, deployment_name)

                # 4. æŸ¥æ‰¾å¹¶åˆ é™¤æºèŠ‚ç‚¹ä¸Šçš„ç›¸å…³ pod
                deleted_pods = await delete_pods_on_node(namespace, deployment_name, source_node)

                results.append(
                    {
                        "namespace": namespace,
                        "deployment": deployment_name,
                        "status": "success",
                        "deleted_pods": deleted_pods,
                    }
                )

            except Exception as e:
                error_message = str(e)
                logger.error(f"è¿ç§» {namespace}.{deployment_name} æ—¶å‡ºé”™: {error_message}")
                results.append(
                    {
                        "namespace": namespace,
                        "deployment": deployment_name,
                        "status": "failed",
                        "error": error_message,
                    }
                )

        return web.json_response(
            {
                "message": f"èŠ‚ç‚¹å‡è¡¡æ“ä½œå®Œæˆ: {source_node} -> {target_node}",
                "success": True,
                "results": results,
            }
        )

    except Exception as e:
        logger.exception(f"èŠ‚ç‚¹å‡è¡¡æ“ä½œå¤±è´¥: {e}")
        return web.json_response({"message": f"æ“ä½œå¤±è´¥: {str(e)}", "success": False}, status=500)


async def remove_node_label(node_name, label_key):
    """ä»èŠ‚ç‚¹åˆ é™¤æŒ‡å®šæ ‡ç­¾"""
    patch_body = {"metadata": {"labels": {label_key: None}}}  # è®¾ç½®æ ‡ç­¾å€¼ä¸º None è¡¨ç¤ºåˆ é™¤æ ‡ç­¾
    try:
        await core_v1.patch_node(name=node_name, body=patch_body)
        logger.info(f"ä»èŠ‚ç‚¹ {node_name} åˆ é™¤æ ‡ç­¾ {label_key} æˆåŠŸ")
    except ApiException as e:
        logger.error(f"ä»èŠ‚ç‚¹ {node_name} åˆ é™¤æ ‡ç­¾ {label_key} æ—¶å‡ºé”™: {e}")
        raise Exception(f"åˆ é™¤æ ‡ç­¾å¤±è´¥: {str(e)}")


async def delete_pods_on_node(namespace, deployment_name, node_name):
    """åˆ é™¤æŒ‡å®šèŠ‚ç‚¹ä¸ŠæŒ‡å®šdeploymentçš„pod"""
    try:
        # è·å–è¯¥namespaceä¸‹çš„æ‰€æœ‰pod
        pods = await core_v1.list_namespaced_pod(namespace=namespace)

        # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒåŒ¹é…deployment_name-[a-z0-9]+-[a-z0-9]+
        pattern = re.compile(f"^{re.escape(deployment_name)}-[a-z0-9]+-[a-z0-9]+$")

        deleted_pods = []
        for pod in pods.items:
            # æ£€æŸ¥podæ˜¯å¦å±äºç›®æ ‡deploymentï¼ˆä½¿ç”¨æ­£åˆ™åŒ¹é…ï¼‰å¹¶ä¸”åœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Š
            if pattern.match(pod.metadata.name) and pod.spec.node_name == node_name:
                logger.info(f"åˆ é™¤pod: {pod.metadata.name}")
                await core_v1.delete_namespaced_pod(name=pod.metadata.name, namespace=namespace)
                deleted_pods.append(pod.metadata.name)

        logger.info(f"åœ¨èŠ‚ç‚¹ {node_name} ä¸Šåˆ é™¤äº† {len(deleted_pods)} ä¸ª {deployment_name} çš„pod")
        return deleted_pods
    except ApiException as e:
        logger.error(f"åˆ é™¤podæ—¶å‡ºé”™: {e}")
        raise Exception(f"åˆ é™¤podå¤±è´¥: {str(e)}")


async def delete_pods_in_available_nodes(namespace, deployment_name, available_nodes):
    """æ ¹æ®namespaceå’Œdeployment_nameç²¾ç¡®æ‰¾åˆ°deploymentçš„æ‰€æœ‰podï¼Œåˆ é™¤åœ¨available_nodesä¸­çš„pod"""
    try:
        # 1. è·å–Deploymentçš„æ ‡ç­¾é€‰æ‹©å™¨
        deployment = await v1.read_namespaced_deployment(deployment_name, namespace)
        selector = deployment.spec.selector.match_labels
        label_selector = ",".join([f"{k}={v}" for k, v in selector.items()])

        deleted_count = 0
        target_delete_count = len(available_nodes)
        deleted_pods = []

        # 2. éå†æ¯ä¸ªavailable_nodeï¼ŒæŸ¥æ‰¾å¹¶åˆ é™¤è¯¥èŠ‚ç‚¹ä¸Šçš„pod
        for node_name in available_nodes:
            if deleted_count >= target_delete_count:
                break

            # ä½¿ç”¨å­—æ®µé€‰æ‹©å™¨è¿‡æ»¤èŠ‚ç‚¹ + æ ‡ç­¾é€‰æ‹©å™¨è¿‡æ»¤Pod
            field_selector = f"spec.nodeName={node_name}"
            pods = await core_v1.list_namespaced_pod(
                namespace=namespace, label_selector=label_selector, field_selector=field_selector
            )

            # 3. åˆ é™¤æ‰¾åˆ°çš„podï¼ˆæ¯ä¸ªèŠ‚ç‚¹åªåˆ é™¤ä¸€ä¸ªpodï¼‰
            for pod in pods.items:
                if deleted_count >= target_delete_count:
                    break

                logger.info(f"åˆ é™¤pod: {pod.metadata.name} (èŠ‚ç‚¹: {pod.spec.node_name})")
                await core_v1.delete_namespaced_pod(name=pod.metadata.name, namespace=namespace)
                deleted_pods.append(pod.metadata.name)
                deleted_count += 1
                break  # æ¯ä¸ªèŠ‚ç‚¹åªåˆ é™¤ä¸€ä¸ªpod

        logger.info(f"åˆ é™¤äº† {deleted_count} ä¸ª {deployment_name} çš„podï¼Œç›®æ ‡åˆ é™¤æ•°é‡: {target_delete_count}")
        return deleted_pods

    except ApiException as e:
        logger.error(f"åˆ é™¤podæ—¶å‡ºé”™: {e}")
        raise Exception(f"åˆ é™¤podå¤±è´¥: {str(e)}")
    except Exception as e:
        logger.error(f"åˆ é™¤podæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise Exception(f"åˆ é™¤podå¤±è´¥: {str(e)}")


def admis_pass(uid):
    return {
        "apiVersion": "admission.k8s.io/v1",
        "kind": "AdmissionReview",
        "response": {"uid": uid, "allowed": True},
    }


def admis_fail(uid, code, message):
    return {
        "apiVersion": "admission.k8s.io/v1",
        "kind": "AdmissionReview",
        "response": {"uid": uid, "allowed": False, "status": {"code": code, "message": message}},
    }


def scale_only(uid, replicas):
    patch_replicas = {"op": "replace", "path": "/spec/replicas", "value": replicas}
    code = base64.b64encode(json.dumps([patch_replicas]).encode()).decode()
    return {
        "apiVersion": "admission.k8s.io/v1",
        "kind": "AdmissionReview",
        "response": {"uid": uid, "allowed": True, "patchType": "JSONPatch", "patch": code},
    }


def get_deployment_affinity(namespace, deployment_name, pod_label):
    affinity = {
        "nodeAffinity": {
            "requiredDuringSchedulingIgnoredDuringExecution": {
                "nodeSelectorTerms": [
                    {
                        "matchExpressions": [
                            {
                                "key": f"{namespace}.{deployment_name}",
                                "operator": "In",
                                "values": [f"{utils.NODE_LABLE_VALUE}"],
                            }
                        ]
                    }
                ]
            }
        },
        "podAntiAffinity": {
            "requiredDuringSchedulingIgnoredDuringExecution": [
                {
                    "labelSelector": {"matchExpressions": [{"key": "app", "operator": "In", "values": [pod_label]}]},
                    "topologyKey": "kubernetes.io/hostname",
                }
            ]
        },
    }
    return affinity


async def get_pod_label_and_maxUnavailable(namespace, deployment_name):
    """ä»deploymentæŸ¥podæ ‡ç­¾å’Œé‡å¯ç­–ç•¥çš„maxUnavailable"""
    try:
        # è·å– Deployment
        deployment = await v1.read_namespaced_deployment(deployment_name, namespace)
        # è·å– app æ ‡ç­¾çš„å€¼
        app_label_value = deployment.spec.template.metadata.labels.get('app')
        # è·å– é‡å¯ç­–ç•¥çš„maxUnavailable
        maxUnavailable_value = deployment.spec.strategy.rolling_update.max_unavailable
        return {"app_label_value": app_label_value, "maxUnavailable_value": maxUnavailable_value}
    except ApiException as e:
        logger.error(f"Kubernetes API é”™è¯¯: {e}")
    except Exception as e:
        logger.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    return None


async def get_deployment_affinity_old(namespace, deployment_name):
    """æ£€æŸ¥deploymentæ˜¯å¦åŒ…å«kubedoor-scheduleræ ‡ç­¾åŒ¹é…
    è¿”å›: has_kubedoor_scheduler
    """
    try:
        # è·å– Deployment
        deployment = await v1.read_namespaced_deployment(deployment_name, namespace)
        # è·å– affinityé…ç½®
        affinity = deployment.spec.template.spec.affinity

        has_kubedoor_scheduler = False

        if affinity and affinity.node_affinity:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«kubedoor-scheduler
            node_affinity = affinity.node_affinity
            if node_affinity.required_during_scheduling_ignored_during_execution:
                node_selector_terms = (
                    node_affinity.required_during_scheduling_ignored_during_execution.node_selector_terms
                )
                for term in node_selector_terms:
                    for expression in term.match_expressions:
                        if 'kubedoor-scheduler' in expression.values:
                            has_kubedoor_scheduler = True
                            break
                    if has_kubedoor_scheduler:
                        break

        return has_kubedoor_scheduler
    except Exception as e:
        logger.error(f"æ£€æŸ¥deployment affinityé…ç½®å¤±è´¥: {e}")
        return False


def process_max_unavailable(max_unavailable):
    if isinstance(max_unavailable, int) or isinstance(max_unavailable, float):
        return max_unavailable
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå¹¶ä¸”åŒ…å« '%'ï¼Œè¡¨ç¤ºæ˜¯ç™¾åˆ†æ¯”
    if '%' in max_unavailable:
        # å»æ‰ç™¾åˆ†å·å¹¶å°†ç™¾åˆ†æ¯”è½¬ä¸ºå°æ•°
        return float(max_unavailable.strip('%')) / 100
    elif '.' in max_unavailable:
        # å°æ•°
        return float(max_unavailable)
    # å¦‚æœæ˜¯æ•´æ•°ï¼Œç›´æ¥è¿”å›åŸå€¼
    else:
        return int(max_unavailable)


async def update_all(
    replicas,
    namespace,
    deployment_name,
    request_cpu_m,
    request_mem_mb,
    limit_cpu_m,
    limit_mem_mb,
    resources_dict,
    uid,
    scheduler,
):
    change_list = []
    scheduler = bool(scheduler)
    if scheduler == True:
        # å¦‚æœå¼€äº†å¼ºåˆ¶è°ƒåº¦å¼€å…³ï¼Œåˆ™è¦è®¾ç½®affinity
        info_dict = await get_pod_label_and_maxUnavailable(namespace, deployment_name)
        if not info_dict:
            logger.error(f"æœªæŸ¥åˆ°ã€{namespace}ã€‘ã€{deployment_name}ã€‘podæ ‡ç­¾æˆ–é‡å¯ç­–ç•¥çš„maxUnavailable")
            return web.json_response({"error": f"æœªæŸ¥åˆ°ã€{namespace}ã€‘ã€{deployment_name}ã€‘podæ ‡ç­¾"}, status=500)
        pod_label = info_dict.get("app_label_value")
        value = get_deployment_affinity(namespace, deployment_name, pod_label)
        logger.info("é…ç½®affinityï¼ˆé€‰æ‹©èŠ‚ç‚¹å’Œåäº²å’Œæ€§ï¼‰")
        affinity = {"op": "replace", "path": "/spec/template/spec/affinity", "value": value}
        logger.info(value)
        change_list.append(affinity)
        maxUnavailable_value = info_dict.get("maxUnavailable_value")
        maxUnavailable_value_new = process_max_unavailable(maxUnavailable_value)
        if int(replicas) * maxUnavailable_value_new < 1:
            logger.info(f"maxUnavailable_valueåŸå€¼ä¸º{maxUnavailable_value}ï¼Œæ”¹ä¸º1")
            maxUnavailable_value = 1
        restart_strategy = {
            "op": "replace",
            "path": "/spec/strategy/rollingUpdate/maxUnavailable",
            "value": maxUnavailable_value,
        }
        change_list.append(restart_strategy)
    else:
        # å¦‚æœdeploymenté…ç½®è¿‡affinityé€‰æ‹©èŠ‚ç‚¹ï¼Œåˆ™åˆ é™¤nodeAffinityéƒ¨åˆ†
        has_kubedoor_scheduler = await get_deployment_affinity_old(namespace, deployment_name)
        if has_kubedoor_scheduler:
            # ç›´æ¥åˆ é™¤nodeAffinityå­—æ®µï¼Œä¿ç•™podAffinityå’ŒpodAntiAffinity
            remove_node_affinity = {"op": "remove", "path": "/spec/template/spec/affinity/nodeAffinity"}
            change_list.append(remove_node_affinity)
            logger.info(
                f"æ£€æŸ¥åˆ°ã€{namespace}ã€‘ã€{deployment_name}ã€‘å·²é…ç½®èŠ‚ç‚¹é€‰æ‹©ï¼Œå¹¶ä¸”è°ƒåº¦å¼€å…³å·²å…³é—­ï¼Œåˆ é™¤nodeAffinityå­—æ®µ"
            )
    # æŒ‰ç…§æ•°æ®åº“ä¿®æ”¹æ‰€æœ‰å‚æ•°
    patch_replicas = {"op": "replace", "path": "/spec/replicas", "value": replicas}
    change_list.append(patch_replicas)
    # request_cpu_m, request_mem_mb, limit_cpu_m, limit_mem_mbï¼Œæœ‰åˆ™æ”¹ï¼Œæ— åˆ™ä¸åŠ¨
    logger.info(f"æ”¹å‰ï¼š{resources_dict}")
    if request_cpu_m > 0:
        resources_dict["requests"]["cpu"] = f"{request_cpu_m}m"
    else:
        utils.send_msg(f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æœªé…ç½® request_cpu_m")
    if request_mem_mb > 0:
        resources_dict["requests"]["memory"] = f"{request_mem_mb}Mi"
    else:
        utils.send_msg(f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æœªé…ç½® request_mem_mb")
    if limit_cpu_m > 0:
        resources_dict["limits"]["cpu"] = f"{limit_cpu_m}m"
    else:
        utils.send_msg(f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æœªé…ç½® limit_cpu_m")
    if limit_mem_mb > 0:
        resources_dict["limits"]["memory"] = f"{limit_mem_mb}Mi"
    else:
        utils.send_msg(f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æœªé…ç½® limit_mem_mb")
    logger.info(f"æ”¹åï¼š{resources_dict}")
    resources = {
        "op": "add",
        "path": "/spec/template/spec/containers/0/resources",
        "value": resources_dict,
    }
    change_list.append(resources)
    code = base64.b64encode(json.dumps(change_list).encode()).decode()
    return {
        "apiVersion": "admission.k8s.io/v1",
        "kind": "AdmissionReview",
        "response": {"uid": uid, "allowed": True, "patchType": "JSONPatch", "patch": code},
    }


async def admis_mutate(request):
    request_info = await request.json()
    object = request_info['request']['object']
    old_object = request_info['request']['oldObject']
    kind = request_info['request']['kind']['kind']
    operation = request_info['request']['operation']
    uid = request_info['request']['uid']
    namespace = object['metadata']['namespace']
    deployment_name = object['metadata']['name']
    logger.info(f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æ”¶åˆ°è¯·æ±‚{object}")

    # æ£€æŸ¥ä¸´æ—¶æ‰©ç¼©å®¹æ ‡è®°
    annotations = object.get('metadata', {}).get('annotations', {})
    scale_temp = annotations.get('scale.temp')
    if scale_temp:
        try:
            # æå–@å‰çš„æ—¶é—´éƒ¨åˆ†
            time_part = scale_temp.split('@')[0]
            # è§£ææ—¶é—´ï¼Œæ³¨æ„æ ¼å¼æ˜¯ "%Y-%m-%d %H:%M:%S"
            temp_time = datetime.strptime(time_part, "%Y-%m-%d %H:%M:%S")
            current_time = datetime.now()
            time_diff = current_time - temp_time

            # å¦‚æœåœ¨5åˆ†é’Ÿå†…ï¼Œå¹¶ä¸”æ»¡è¶³ç‰¹å®šæ¡ä»¶ï¼Œç›´æ¥æ”¾è¡Œ
            if time_diff <= timedelta(minutes=5):
                if (kind == 'Scale' and operation == 'UPDATE') or (
                    kind == 'Deployment'
                    and operation == 'UPDATE'
                    and object['spec']['template'] == old_object['spec']['template']
                    and old_object['spec']['replicas'] != object['spec']['replicas']
                ):
                    logger.info(
                        f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘ä¸´æ—¶æ‰©ç¼©å®¹åœ¨5åˆ†é’Ÿå†…ï¼Œç›´æ¥æ”¾è¡Œ: {scale_temp}"
                    )
                    return web.json_response(admis_pass(uid))
        except Exception as e:
            logger.warning(f"è§£æscale.tempæ—¶é—´å¤±è´¥: {e}")

    if ws_conn is None or ws_conn.closed:
        utils.send_msg(
            f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘è¿æ¥ kubedoor-master å¤±è´¥"
        )
        return web.json_response(admis_fail(uid, 503, "è¿æ¥ kubedoor-master å¤±è´¥"))

    response_future = asyncio.get_event_loop().create_future()
    request_futures[uid] = response_future
    await ws_conn.send_json({"type": "admis", "request_id": uid, "namespace": namespace, "deployment": deployment_name})
    try:
        result = await asyncio.wait_for(response_future, timeout=30)
        logger.info(f"response_future æ”¶åˆ° admis å“åº”ï¼š{uid} {result}")
    except asyncio.TimeoutError:
        del request_futures[uid]
        utils.send_msg(
            f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘è¿æ¥ kubedoor-master å“åº”è¶…æ—¶"
        )
        return web.json_response(admis_fail(uid, 504, "ç­‰å¾… kubedoor-master å“åº”è¶…æ—¶"))

    if len(result) == 2:
        utils.send_msg(f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘{result[1]}")
        if result[0] == 200:
            return web.json_response(admis_pass(uid))
        return web.json_response(admis_fail(uid, result[0], result[1]))

    (
        pod_count,
        pod_count_ai,
        pod_count_manual,
        request_cpu_m,
        request_mem_mb,
        limit_cpu_m,
        limit_mem_mb,
        scheduler,
    ) = result
    # å‰¯æœ¬æ•°å–å€¼ä¼˜å…ˆçº§ï¼šä¸“å®¶å»ºè®®â†’aiå»ºè®®â†’åŸæœ¬çš„å‰¯æœ¬æ•°
    replicas = pod_count_manual if pod_count_manual >= 0 else (pod_count_ai if pod_count_ai >= 0 else pod_count)
    # å¦‚æœæ•°æ®åº“ä¸­request_cpu_mä¸º0ï¼Œè®¾ç½®ä¸º10ï¼›å¦‚æœrequest_mem_mbä¸º0ï¼Œè®¾ç½®ä¸º1
    request_cpu_m = 10 if 0 <= request_cpu_m < 10 else request_cpu_m
    request_mem_mb = 1 if request_mem_mb == 0 else request_mem_mb
    deploy_baseinfo = f"å‰¯æœ¬æ•°:{replicas}, è¯·æ±‚CPU:{request_cpu_m}m, è¯·æ±‚å†…å­˜:{request_mem_mb}MB, é™åˆ¶CPU:{limit_cpu_m}m, é™åˆ¶å†…å­˜:{limit_mem_mb}MB"
    logger.info(deploy_baseinfo)

    try:
        if kind == 'Scale' and operation == 'UPDATE':
            # æŠŠspec.replicasä¿®æ”¹æˆæ•°æ®åº“çš„podæ•°ä¸€è‡´
            admis_msg = f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æ”¶åˆ°scaleè¯·æ±‚ï¼Œä»…ä¿®æ”¹replicasä¸º: {replicas}"
            logger.info(admis_msg)
            utils.send_msg(admis_msg)
            return web.json_response(scale_only(uid, replicas))
        elif kind == 'Deployment' and operation == 'CREATE':
            # æŒ‰ç…§æ•°æ®åº“ä¿®æ”¹æ‰€æœ‰å‚æ•°
            admis_msg = f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æ”¶åˆ° create è¯·æ±‚ï¼Œä¿®æ”¹æ‰€æœ‰å‚æ•°"
            logger.info(admis_msg)
            utils.send_msg(f'{admis_msg}\n{deploy_baseinfo}\nå›ºå®šèŠ‚ç‚¹å‡è¡¡: {scheduler}')
            resources_dict = object['spec']['template']['spec']['containers'][0].get('resources', {}) or {}
            # ç¡®ä¿ requests å’Œ limits å­—æ®µéƒ½å­˜åœ¨
            resources_dict.setdefault('requests', {})
            resources_dict.setdefault('limits', {})
            return web.json_response(
                await update_all(
                    replicas,
                    namespace,
                    deployment_name,
                    request_cpu_m,
                    request_mem_mb,
                    limit_cpu_m,
                    limit_mem_mb,
                    resources_dict,
                    uid,
                    scheduler,
                )
            )
        elif kind == 'Deployment' and operation == 'UPDATE':
            template = object['spec']['template']
            old_template = old_object['spec']['template']
            if object == old_object:
                logger.info(
                    f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘object å’Œ oldObject ç›¸ç­‰ï¼Œè·³è¿‡"
                )
                return web.json_response(admis_pass(uid))
            elif template != old_template:
                # spec.template å˜äº†,è§¦å‘é‡å¯é€»è¾‘,æŒ‰ç…§æ•°æ®åº“ä¿®æ”¹æ‰€æœ‰å‚æ•°
                admis_msg = f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æ”¶åˆ° update è¯·æ±‚ï¼Œä¿®æ”¹æ‰€æœ‰å‚æ•°"
                logger.info(admis_msg)
                utils.send_msg(f'{admis_msg}\n{deploy_baseinfo}\nå›ºå®šèŠ‚ç‚¹å‡è¡¡: {scheduler}')
                resources_dict = object['spec']['template']['spec']['containers'][0].get('resources', {}) or {}
                # ç¡®ä¿ requests å’Œ limits å­—æ®µéƒ½å­˜åœ¨
                resources_dict.setdefault('requests', {})
                resources_dict.setdefault('limits', {})
                return web.json_response(
                    await update_all(
                        replicas,
                        namespace,
                        deployment_name,
                        request_cpu_m,
                        request_mem_mb,
                        limit_cpu_m,
                        limit_mem_mb,
                        resources_dict,
                        uid,
                        scheduler,
                    )
                )
            elif template == old_template and replicas != object['spec']['replicas']:
                # spec.template æ²¡å˜,spec.replicas å˜äº†,åªæŠŠä¿®æ”¹spec.replicaså’Œæ•°æ®åº“çš„podæ•°ä¸€è‡´
                admis_msg = f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘æ”¶åˆ° update è¯·æ±‚ï¼Œä»…ä¿®æ”¹replicasä¸º: {replicas}"
                logger.info(admis_msg)
                utils.send_msg(admis_msg)
                return web.json_response(scale_only(uid, replicas))
            elif template == old_template and replicas == object['spec']['replicas']:
                # spec.template æ²¡å˜,spec.replicas æ²¡å˜,ä»€ä¹ˆä¹Ÿä¸åš
                logger.info(
                    f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘template å’Œ replicas æ²¡å˜ï¼Œä¸å¤„ç†"
                )
                return web.json_response(admis_pass(uid))
        else:
            admis_msg = f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘ä¸ç¬¦åˆé¢„è®¾åˆ¤æ–­æ¡ä»¶: {kind} {operation}ï¼Œç›´æ¥æ”¾è¡Œ"
            logger.error(admis_msg)
            utils.send_msg(admis_msg)
            return web.json_response(admis_pass(uid))
    except Exception as e:
        logger.exception(f"ã€{namespace}ã€‘ã€{deployment_name}ã€‘Webhook å¤„ç†é”™è¯¯ï¼š{e}")
        utils.send_msg(f"admis:ã€{utils.PROM_K8S_TAG_VALUE}ã€‘ã€{namespace}ã€‘ã€{deployment_name}ã€‘å¤„ç†é”™è¯¯ï¼š{e}")
        return web.json_response({"error": str(e)}, status=500)


async def get_labeled_nodes_count(namespace, deployment_name, nodes):
    """è·å–å·²æœ‰æŒ‡å®šæ ‡ç­¾çš„èŠ‚ç‚¹æ•°"""
    labeled_nodes_count = 0
    label_key = f"{namespace}.{deployment_name}"
    for node in nodes.items:
        labels = node.metadata.labels
        if labels and labels.get(label_key) == utils.NODE_LABLE_VALUE:
            labeled_nodes_count += 1
    return labeled_nodes_count


async def delete_label(namespace, deployment_name, nodes):
    """å¦‚æœèŠ‚ç‚¹ä¸Šæ²¡æœ‰è¿™ä¸ªæœåŠ¡çš„podï¼Œåˆ™åˆ æ‰æ ‡ç­¾"""
    label_key = f"{namespace}.{deployment_name}"
    for node in nodes.items:
        node_name = node.metadata.name
        labels = node.metadata.labels
        flag = False
        if labels and labels.get(label_key) == utils.NODE_LABLE_VALUE:
            try:
                # è·å–èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰ Pod
                all_pods = await core_v1.list_pod_for_all_namespaces(field_selector=f"spec.nodeName={node_name}")
                for pod in all_pods.items:
                    pod_name = pod.metadata.name
                    # åˆ¤æ–­ Pod æ˜¯å¦å±äºæŒ‡å®šçš„æœåŠ¡ (æ­¤é€»è¾‘å¯æ ¹æ®å…·ä½“æœåŠ¡æ ‡ç­¾è°ƒæ•´)
                    if pod_name and pod_name.startswith(f"{deployment_name}-"):
                        flag = True
                        break
            except ApiException as e:
                logger.error(f"æ£€æŸ¥èŠ‚ç‚¹ {node_name} çš„æœåŠ¡ Pod æ—¶å‡ºç°é—®é¢˜: {e}")
            if not flag:
                patch_body = {"metadata": {"labels": {label_key: None}}}  # è®¾ç½®æ ‡ç­¾å€¼ä¸º None è¡¨ç¤ºåˆ é™¤æ ‡ç­¾
                try:
                    await core_v1.patch_node(name=node_name, body=patch_body)
                    logger.info(f"èŠ‚ç‚¹ {node_name}ä¸Šæœªéƒ¨ç½²æœåŠ¡{deployment_name}ï¼Œå·²åˆ é™¤æ ‡ç­¾ {label_key}")
                except ApiException as e:
                    logger.error(f"åˆ é™¤èŠ‚ç‚¹ {node_name} ä¸Šæ ‡ç­¾ {label_key} æ—¶å‡ºé”™: {e}")


async def select_least_loaded_nodes(namespace, nodes_to_label_count, deployment_name, node_cpu_list):
    """é€‰æ‹© CPU ä½¿ç”¨ç‡æœ€ä½çš„èŠ‚ç‚¹å¹¶è¿”å›"""
    nodes = await core_v1.list_node()
    node_filter_list = []
    sorted_nodes = []

    for node in nodes.items:
        is_scheduled = True
        # taints = node.spec.taints if node.spec.taints else []
        # for taint in taints:
        #     # å¦‚æœèŠ‚ç‚¹ä¸Šå­˜åœ¨ NoSchedule æˆ– PreferNoSchedule çš„ taintï¼Œè¯¥èŠ‚ç‚¹ä¸å¯è°ƒåº¦
        #     if taint.effect in ['NoSchedule', 'PreferNoSchedule']:
        #         is_scheduled = False
        #         break
        # è¿‡æ»¤æ‰å·²åŠ è¿‡è¯¥æœåŠ¡æ ‡ç­¾çš„èŠ‚ç‚¹
        labels = node.metadata.labels
        label_key = f"{namespace}.{deployment_name}"
        if labels and labels.get(label_key) == utils.NODE_LABLE_VALUE:
            continue
        if is_scheduled:
            node_filter_list.append(node.metadata.name)
    logger.info(f"ã€æ‰©å®¹(ä½-->é«˜)ã€‘node_cpu_list: {node_cpu_list}")
    for i in node_cpu_list:
        if i.get('name') in node_filter_list:
            sorted_nodes.append(i.get('name'))

    # è¿”å› CPU ä½¿ç”¨ç‡æœ€ä½çš„èŠ‚ç‚¹
    if len(sorted_nodes) >= nodes_to_label_count:
        return sorted_nodes[:nodes_to_label_count]
    else:
        return None


async def select_del_label_nodes(namespace, del_label_count, deployment_name, node_cpu_list):
    """é€‰æ‹© CPU ä½¿ç”¨ç‡æœ€é«˜çš„èŠ‚ç‚¹å¹¶è¿”å›"""
    nodes = await core_v1.list_node()
    node_filter_list = []
    sorted_nodes = []

    for node in nodes.items:
        labels = node.metadata.labels
        label_key = f"{namespace}.{deployment_name}"
        if labels and labels.get(label_key) == utils.NODE_LABLE_VALUE:
            node_filter_list.append(node.metadata.name)

    # æŒ‰ç…§percentå€¼å€’åºæ’åˆ—ï¼Œé€‰æ‹©CPUä½¿ç”¨ç‡æœ€é«˜çš„èŠ‚ç‚¹
    sorted_cpu_list = sorted(node_cpu_list, key=lambda x: x.get('percent', 0), reverse=True)
    logger.info(f"ã€ç¼©å®¹(é«˜-->ä½)ã€‘node_cpu_list: {sorted_cpu_list}")
    for i in sorted_cpu_list:
        if i.get('name') in node_filter_list:
            sorted_nodes.append(i.get('name'))

    # è¿”å› CPU ä½¿ç”¨ç‡æœ€é«˜çš„èŠ‚ç‚¹
    return sorted_nodes[:del_label_count]


async def del_node_with_label(namespace, node_name, deployment_name):
    """ä¸ºèŠ‚ç‚¹åˆ é™¤æ ‡ç­¾"""
    label_key = f"{namespace}.{deployment_name}"  # ä½¿ç”¨ å‘½åç©ºé—´.éƒ¨ç½²åç§° ä½œä¸ºæ ‡ç­¾é”®
    patch_body = {"metadata": {"labels": {label_key: None}}}
    try:
        await core_v1.patch_node(name=node_name, body=patch_body)
        logger.info(f"èŠ‚ç‚¹ {node_name} ä¸Šå·²åˆ é™¤æ ‡ç­¾ {label_key}")
    except ApiException as e:
        logger.error(f"åœ¨èŠ‚ç‚¹ {node_name} ä¸Šæ›´æ–°æ ‡ç­¾æ—¶å‡ºé”™: {e}")


async def update_node_with_label(namespace, node_name, deployment_name):
    """ä¸ºèŠ‚ç‚¹æ·»åŠ æ ‡ç­¾"""
    label_key = f"{namespace}.{deployment_name}"  # ä½¿ç”¨ å‘½åç©ºé—´.éƒ¨ç½²åç§° ä½œä¸ºæ ‡ç­¾é”®
    patch_body = {"metadata": {"labels": {label_key: utils.NODE_LABLE_VALUE}}}
    try:
        await core_v1.patch_node(name=node_name, body=patch_body)
        logger.info(f"èŠ‚ç‚¹ {node_name} ä¸Šå·²æ·»åŠ æ ‡ç­¾ {label_key}={utils.NODE_LABLE_VALUE}")
    except ApiException as e:
        logger.error(f"åœ¨èŠ‚ç‚¹ {node_name} ä¸Šæ›´æ–°æ ‡ç­¾æ—¶å‡ºé”™: {e}")


async def setup_routes(app):
    app.router.add_get('/api/health', health_check)
    app.router.add_post('/api/update-image', update_image)
    app.router.add_post('/api/scale', scale)
    app.router.add_post('/api/restart', reboot)
    app.router.add_post('/api/cron', cron)
    app.router.add_get('/api/admis_switch', admis_switch)
    app.router.add_post('/api/admis', admis_mutate)
    app.router.add_get('/api/events', get_namespace_events)
    app.router.add_get('/api/get_dpm_pods', get_deployment_pods)
    app.router.add_get('/api/nodes', get_nodes_info)
    app.router.add_post('/api/balance_node', balance_node)
    # nodeç®¡ç†æ¥å£
    app.router.add_get('/api/nodes/list', lambda request: get_nodes_list(core_v1, custom_api, request))
    app.router.add_post('/api/nodes/cordon', lambda request: cordon_nodes(core_v1, request))
    app.router.add_post('/api/nodes/uncordon', lambda request: uncordon_nodes(core_v1, request))
    # ConfigMapç®¡ç†æ¥å£
    app.router.add_get('/api/agent/configmaps', lambda request: configmap_manager.get_configmap_list(core_v1, request))
    # Serviceç®¡ç†æ¥å£
    app.router.add_get('/api/agent/services', lambda request: service_manager.get_service_list(core_v1, request))
    app.router.add_get(
        '/api/agent/service/endpoints', lambda request: service_manager.get_service_endpoints(core_v1, request)
    )
    app.router.add_get(
        '/api/agent/service/first-port', lambda request: service_manager.get_service_first_port(core_v1, request)
    )
    # Ingressç®¡ç†æ¥å£
    app.router.add_get('/api/agent/ingresses', lambda request: ingress_manager.get_ingress_list(networking_v1, request))
    app.router.add_get(
        '/api/agent/ingress/rules',
        lambda request: ingress_manager.get_ingress_rules(custom_api, request),
    )
    # Podç®¡ç†æ¥å£
    app.router.add_get('/api/agent/pods', lambda request: pod_manager.get_pod_list(core_v1, custom_api, request))
    # VirtualServiceç®¡ç†æ¥å£
    app.router.add_get('/api/agent/istio/vs', lambda request: istio_manager.get_virtualservice(custom_api, request))
    app.router.add_post(
        '/api/agent/istio/vs/apply', lambda request: istio_manager.apply_virtualservice(custom_api, request)
    )
    # app.router.add_delete('/api/agent/istio/vs/delete', lambda request: istio_manager.delete_virtualservice(custom_api, request))

    # K8Sèµ„æºç®¡ç†æ¥å£
    app.router.add_post('/api/agent/res/ops', k8s_resource_handler.handle_k8s_operation)
    app.router.add_get('/api/agent/res/content', k8s_resource_handler.handle_get_resource_content)
    app.router.add_delete('/api/agent/res/delete', k8s_resource_handler.handle_delete_resource)

    # StatefulSetç®¡ç†æ¥å£
    app.router.add_get(
        '/api/agent/statefulsets', lambda request: stateful_daemon_manager.get_statefulset_list(v1, request)
    )
    app.router.add_get(
        '/api/agent/statefulset/pods',
        lambda request: stateful_daemon_manager.get_statefulset_pods(request, core_v1, custom_api, v1),
    )
    app.router.add_post(
        '/api/agent/statefulset/restart', lambda request: stateful_daemon_manager.restart_statefulset(request, v1)
    )
    app.router.add_post(
        '/api/agent/statefulset/scale', lambda request: stateful_daemon_manager.scale_statefulset(request, v1)
    )

    # DaemonSetç®¡ç†æ¥å£
    app.router.add_get('/api/agent/daemonsets', lambda request: stateful_daemon_manager.get_daemonset_list(v1, request))
    app.router.add_get(
        '/api/agent/daemonset/pods',
        lambda request: stateful_daemon_manager.get_daemonset_pods(request, core_v1, custom_api, v1),
    )
    app.router.add_post(
        '/api/agent/daemonset/restart', lambda request: stateful_daemon_manager.restart_daemonset(request, v1)
    )


async def start_https_server():
    """å¯åŠ¨ HTTPS æœåŠ¡å™¨"""
    app = web.Application()
    await setup_routes(app)
    import ssl

    ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
    ssl_context.load_cert_chain('/serving-certs/tls.crt', '/serving-certs/tls.key')

    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 443, ssl_context=ssl_context)
    await site.start()
    logger.info("HTTPS æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ 443")
    while True:
        await asyncio.sleep(3600)


async def main():
    """ä¸»å‡½æ•°"""
    init_kubernetes()  # åˆå§‹åŒ– Kubernetes é…ç½®
    await asyncio.gather(connect_to_server(), start_https_server())


if __name__ == "__main__":
    asyncio.run(main())
